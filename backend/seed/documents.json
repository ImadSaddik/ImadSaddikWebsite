[
  {
    "id": "0ad476f1-7032-43aa-a602-4ae726e19643",
    "name": "elasticsearch-change-heap-size",
    "tags": ["Elasticsearch"],
    "creation_date": 1755730800,
    "year": "2025",
    "title": "Change the heap size for Elasticsearch",
    "type": "blog-post",
    "content": "# Change the heap size for Elasticsearch\n\nHow to change the heap size for Elasticsearch to improve performance and reduce memory usage.\n\n**Date:** August 21, 2025\n**Tags:** Elasticsearch\n\n-----\n\n## Introduction\n\nIn this article, I'll show you how to change the **heap size**. For a more detailed walkthrough, check out the video tutorial:\n\nhttps://www.youtube.com/watch?v=guQuHmpPMXs\n\nYou can also find all related notebooks and slides in my [GitHub repository](https://github.com/ImadSaddik/ElasticSearch_Python_Course).\n\n-----\n\n## Heap size\n\nElasticsearch is a powerful tool, but it can use a lot of resources by default. If you're using it for small projects, you'll want to configure it to use less memory by adjusting the **heap size**.\n\nHeap size is the amount of memory given to an application to use while it's running. By default, **Elasticsearch** takes 50% of your system's available memory. For example, my system has **16GB** of RAM, so whenever I start the container, **Elasticsearch** uses **8GB**. This can slow down my PC a lot.\n\nFor small projects and local setups, just **1â€“2GB** is usually enough.\n\n-----\n\n## Changing the heap size\n\nFollow these steps to adjust the heap size.\n\n### Start the container\n\nOpen your terminal and run:\n\n```bash\nsudo docker start elasticsearch\n```\n\n> **Note:** If you gave your container a different name, replace `elasticsearch` with that name in the command.\n\n### Access the container\n\nRun the following command to enter the `elasticsearch` container:\n\n```bash\nsudo docker exec -u 0 -it elasticsearch bash\n```\n\n### Create the \"heap.options\" file\n\nGo to the `jvm.options.d` folder and set the **heap size**. Run these commands to create the file and set the memory limits:\n\n```bash\necho \"-Xms2g\" > /usr/share/elasticsearch/config/jvm.options.d/heap.options\necho \"-Xmx2g\" >> /usr/share/elasticsearch/config/jvm.options.d/heap.options\n```\n\nCheck the file's contents with:\n\n```bash\ncat /usr/share/elasticsearch/config/jvm.options.d/heap.options\n```\n\nYou should see this in the output:\n\n```text\n-Xms2g\n-Xmx2g\n```\n\nIn this example, **Elasticsearch** is set to use **2GB** of memory. You can change the value by replacing `2` with the amount you want.\n\n-----\n\n## Conclusion\n\nChanging the **heap size** for **Elasticsearch** is key to making it work well for your needs, especially for smaller projects. By following these steps, you can easily adjust memory usage and keep your system running smoothly.\n",
    "view_count": 7,
    "read_count": 3,
    "claps_count": 0
  },
  {
    "id": "556b91f6-a555-46c1-a511-4d4ebd6d01a4",
    "name": "elasticsearch-collapse-search-results",
    "tags": ["Elasticsearch"],
    "creation_date": 1755644400,
    "year": "2025",
    "title": "Collapse search results in Elasticsearch",
    "type": "blog-post",
    "content": "# Collapse search results in Elasticsearch\n\nHow to show only the best documents for each group with collapsing.\n\n**Date:** August 20, 2025\n**Tags:** Elasticsearch\n\n-----\n\n## Introduction\n\nIn this article, I'll show you how to collapse search results. For a more detailed walkthrough, check out the video tutorial:\n\n[[https://www.youtube.com/embed/znhN54KVqbY](https://www.youtube.com/embed/znhN54KVqbY)](https://www.youtube.com/watch?v=znhN54KVqbY)\n\nYou can also find all related notebooks and slides in my [GitHub repository](https://github.com/ImadSaddik/ElasticSearch_Python_Course).\n\n-----\n\n## Collapse search results\n\n**Collapsing** is a cool feature that helps you show only the best document for each **group**. A group is a unique value in a field. Let's index some documents to show how collapse works in Elasticsearch.\n\n### Indexing documents\n\nLet's use the `Apod` dataset, which you can find in [this GitHub repository](https://github.com/ImadSaddik/ElasticSearch_Python_Course/blob/main/data/apod.json). Start by reading the file.\n\n```python\nimport json\n\nwith open(\"../data/apod.json\") as f:\n    documents = json.load(f)\n```\n\nThen, create an index.\n\n```python\nfrom elasticsearch import Elasticsearch\n\nes = Elasticsearch(\"http://localhost:9200\")\n\nes.indices.delete(index=\"apod\", ignore_unavailable=True)\nes.indices.create(index=\"apod\")\n```\n\nUse the `bulk API` to index the documents in the `apod index`.\n\n```python\nfrom tqdm import tqdm\n\noperations = []\nindex_name = \"apod\"\nfor document in tqdm(documents, total=len(documents), desc=\"Indexing documents\"):\n    year = document[\"date\"].split(\"-\")[0]\n    document[\"year\"] = int(year)\n\n    operations.append({\"index\": {\"_index\": index_name}})\n    operations.append(document)\n\nresponse = es.bulk(operations=operations)\nresponse[\"errors\"]\n```\n\nIf the indexing is successful, you should see `response[\"errors\"]` as `False`.\n\n### Collapsing\n\nNow, let's search for documents where `Andromeda galaxy` appears in the title.\n\n```python\nresponse_no_collapsing = es.search(\n    index=\"apod\",\n    body={\n        \"query\": {\"match\": {\"title\": \"Andromeda galaxy\"}},\n        \"size\": 10_000,\n    },\n)\ntotal_hits = response_no_collapsing[\"hits\"][\"total\"][\"value\"]\nprint(f\"Total hits before collapsing: {total_hits}\")\ntotal_returned_hits = len(response_no_collapsing[\"hits\"][\"hits\"])\nprint(f\"Total returned hits before collapsing: {total_returned_hits}\")\n```\n\nWithout collapsing, the search results will return all documents that match the query. In this case, the number is 270 documents.\n\n```text\nTotal hits before collapsing: 270\nTotal returned hits before collapsing: 270\n```\n\nLet's look at the count of documents that matched the query per year in the `apod index`.\n\n```python\nfrom elastic_transport import ObjectApiResponse\n\ndef get_hits_per_year(response: ObjectApiResponse) -> dict:\n    hits_per_year_count = {}\n    for hit in response[\"hits\"][\"hits\"]:\n        year = hit[\"_source\"][\"year\"]\n        if year not in hits_per_year_count:\n            hits_per_year_count[year] = 0\n        hits_per_year_count[year] += 1\n    return hits_per_year_count\n\nprint(\"Hits per year count:\")\nprint(get_hits_per_year(response_no_collapsing))\n```\n\nWe see that we have a lot of documents per year. What would happen if we collapse the search results by year?\n\n```text\nHits per year count:\n{\n    2015: 28,\n    2016: 29,\n    2017: 31,\n    2018: 19,\n    2019: 32,\n    2020: 25,\n    2021: 24,\n    2022: 30,\n    2023: 32,\n    2024: 20,\n}\n```\n\nCollapsing search results by year will return only one document per year that matches the query. That returned document will be the one with the highest `_score` for that year.\n\n```python\nresponse_collapsing = es.search(\n    index=\"apod\",\n    body={\n        \"query\": {\"match\": {\"title\": \"Andromeda galaxy\"}},\n        \"collapse\": {\"field\": \"year\"},\n        \"size\": 10_000,\n    },\n)\ntotal_hits = response_collapsing[\"hits\"][\"total\"][\"value\"]\nprint(f\"Total hits before collapsing: {total_hits}\")\ntotal_returned_hits = len(response_collapsing[\"hits\"][\"hits\"])\nprint(f\"Total returned hits after collapsing: {total_returned_hits}\")\n```\n\nNow, we got 10 documents after collapsing the search results.\n\n```text\nTotal hits before collapsing: 270\nTotal returned hits after collapsing: 10\n```\n\nLet's print the number of documents per year.\n\n```python\nprint(\"Hits per year count:\")\nprint(get_hits_per_year(response_collapsing))\n```\n\nNow we have only one document per year that matches the query.\n\n```text\nHits per year count:\n{\n    2015: 1,\n    2016: 1,\n    2017: 1,\n    2018: 1,\n    2019: 1,\n    2020: 1,\n    2021: 1,\n    2022: 1,\n    2023: 1,\n    2024: 1,\n}\n```\n\nLet's check if the document in year 2024 is the one with the highest `_score`.\n\n```python\nfor hit in response_collapsing[\"hits\"][\"hits\"]:\n    year = hit[\"_source\"][\"year\"]\n    if year == 2024:\n        score = hit[\"_score\"]\n        print(f\"Document with a score of {score} for year {year}:\")\n        print(hit[\"_source\"])\n        break\n```\n\nFrom the response with collapsing, we can see that the document in year 2024 has a `_score` of `7.789091`.\n\n```text\nDocument with a score of 7.789091 for year 2024:\n\n{\n    \"authors\": \"Subaru, Hubble, Mayall, R. Gendler, R. Croman\",\n    \"date\": \"2024-09-08\",\n    \"explanation\": \"Explanation: The most distant object easily visible to the unaided eye is M31, the great Andromeda Galaxy...\",\n    \"image_url\": \"https://apod.nasa.gov/apod/image/2409/M31_HstSubaruGendler_960.jpg\",\n    \"title\": \"M31: The Andromeda Galaxy\",\n    \"year\": 2024,\n}\n```\n\nLet's look at the scores in the response without collapsing.\n\n```python\nfor hit in response_no_collapsing[\"hits\"][\"hits\"]:\n    year = hit[\"_source\"][\"year\"]\n    if year == 2024:\n        score = hit[\"_score\"]\n        print(f\"Score {score}:\")\n        print(hit[\"_source\"])\n        print(\"-\" * 50)\n```\n\nWe confirm that the first hit from 2024 has a `_score` of `7.789091`, which is the same as the one in the response with collapsing.\n\n```text\nScore 7.789091:\n{\n    \"authors\": \"Subaru, Hubble, Mayall, R. Gendler, R. Croman\",\n    \"date\": \"2024-09-08\",\n    \"explanation\": \"Explanation: The most distant object easily visible to the unaided eye is M31, the great Andromeda Galaxy...\",\n    \"image_url\": \"https://apod.nasa.gov/apod/image/2409/M31_HstSubaruGendler_960.jpg\",\n    \"title\": \"M31: The Andromeda Galaxy\",\n    \"year\": 2024,\n}\n--------------------------------------------------\nScore 3.0710938:\n{\n    \"authors\": \"Aman Chokshi\",\n    \"date\": \"2024-07-14\",\n    \"explanation\": \"Explanation: The galaxy was never in danger...\",\n    \"image_url\": \"https://apod.nasa.gov/apod/image/2407/M33Meteor_Chokshi_960.jpg\",\n    \"title\": \"Meteor Misses Galaxy\",\n    \"year\": 2024,\n}\n--------------------------------------------------\n...\n```\n\n### Expanding collapsed results\n\nExpanding collapsed results allows you to get more than one document per year that matches the query. To expand the results, we add `inner_hits` to `collapse`. The name `most_recent` will be used to extract those documents from the response object.\n\n```python\nresponse_collapsing = es.search(\n    index=\"apod\",\n    body={\n        \"query\": {\"match\": {\"title\": \"Andromeda galaxy\"}},\n        \"collapse\": {\n            \"field\": \"year\",\n            \"inner_hits\": {\n                \"name\": \"most_recent\",\n                \"size\": 3,  # Number of documents to return per collapsed group\n            },\n        },\n        \"size\": 10_000,\n    },\n)\ntotal_hits = response_collapsing[\"hits\"][\"total\"][\"value\"]\nprint(f\"Total hits before collapsing: {total_hits}\")\ntotal_returned_hits = len(response_collapsing[\"hits\"][\"hits\"])\nprint(f\"Total returned hits after collapsing: {total_returned_hits}\")\ninner_hits = response_collapsing[\"hits\"][\"hits\"][0][\"inner_hits\"][\"most_recent\"]\ntotal_returned_hits_after_expanding = len(inner_hits[\"hits\"][\"hits\"])\nprint(f\"Total returned hits after expanding: {total_returned_hits_after_expanding}\")\n```\n\nFor each year, we get 3 documents inside the `inner_hits` field.\n\n```text\nTotal hits before collapsing: 270\nTotal returned hits after collapsing: 10\nTotal returned hits after expanding: 3\n```\n\nLet's see how many documents we have in a single year.\n\n```python\nprint(\"Hits per year count:\")\nprint(get_hits_per_year(inner_hits))\n```\n\nWe get 3 documents.\n\n```text\nHits per year count:\n{2024: 3}\n```\n\nThe documents are sorted by `_score` within each collapsed group. They also match the scores in the response without collapsing.\n\n```python\nfor hit in inner_hits[\"hits\"][\"hits\"]:\n    score = hit[\"_score\"]\n    print(f\"Score: {score}\")\n```\n\n```text\nScore: 7.789091\nScore: 3.0710938\nScore: 2.792822\n```\n\n### Collapsing with search_after\n\nWhen collapsing on a field with a lot of unique values, you can use the `search_after` parameter to paginate through the results.\n\n> **Note:** You can't use the `scroll API` with collapsing. Use `search_after` instead.\n\nLet's create a new index where we are going to create 40,000 documents. Each user will have 2 documents.\n\n```python\ndocuments = []\nnumber_of_unique_user_ids = 20_000\nfor user_id in range(number_of_unique_user_ids):\n    for i in range(2):\n        documents.append(\n            {\n                \"user_id\": user_id,\n                \"title\": f\"Document {i} for user {user_id}\",\n                \"content\": f\"This is the content of document {i} for user {user_id}.\",\n            }\n        )\n\nes.indices.delete(index=\"my_index\", ignore_unavailable=True)\nes.indices.create(index=\"my_index\")\n\noperations = []\nfor document in tqdm(documents, total=len(documents), desc=\"Indexing documents\"):\n    operations.append({\"index\": {\"_index\": \"my_index\"}})\n    operations.append(document)\n\nresponse = es.bulk(operations=operations)\nresponse[\"errors\"]\n```\n\n```text\nIndexing documents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40000/40000 [00:00<00:00, 1809508.07it/s]\n\nFalse\n```\n\nLet's confirm that we have 40,000 documents in the index.\n\n```python\ndocument_count = es.count(index=\"my_index\")\nprint(f\"Total documents indexed: {document_count['count']}\")\n```\n\n```text\nTotal documents indexed: 40000\n```\n\nNow we're ready to use `search_after` to paginate through the collapsed results. Since we have 2 documents per user, we can expect to have 20,000 collapsed results.\n\n```python\ncollapsed_hits = []\nsearch_after = None\n\nwhile True:\n    body = {\n        \"query\": {\"match\": {\"content\": \"document\"}},\n        \"collapse\": {\"field\": \"user_id\"},\n        \"sort\": [\"user_id\"],\n        \"size\": 10_000,\n    }\n\n    if search_after is not None:\n        body[\"search_after\"] = [search_after]\n\n    response_collapsing = es.search(index=\"my_index\", body=body)\n    hits = response_collapsing[\"hits\"][\"hits\"]\n\n    if not hits:\n        break\n\n    search_after = hits[-1][\"_source\"][\"user_id\"]\n    print(f\"Last user ID: {search_after}\")\n\n    collapsed_hits.extend(hits)\n\nprint(f\"Total collapsed hits: {len(collapsed_hits)}\")\n```\n\nWe see that the last user ID in the collapsed results is `19999` and the number of collapsed hits is `20000`, which is what we expected.\n\n```text\nLast user ID: 9999\nLast user ID: 19999\nTotal collapsed hits: 20000\n```\n\n-----\n\n## Conclusion\n\n In this article, we explored how `collapsing` search results works in `Elasticsearch`. This feature helps you refine your search output by showing only the most relevant document from each `group`, like getting just one top result per year.\n\nWe learned how `collapsing` reduces the number of returned hits for a cleaner view, and how `inner_hits` can expand those results to show more details within each group.\n\nFor large datasets, we also saw that `search_after` is the way to paginate through collapsed results, especially when the `scroll API` isn't suitable. Understanding `collapsing` can really help you make your `Elasticsearch` searches more efficient and your data easier to work with.\n",
    "view_count": 7,
    "read_count": 4,
    "claps_count": 0
  },
  {
    "id": "fcfda572-d381-44fe-ab92-d244c35d10f0",
    "name": "star-trails-with-smartphone",
    "tags": [
      "Astronomy",
      "Star trails",
      "Timelapse",
      "Sky Tonight",
      "Astrophotography"
    ],
    "creation_date": 1760655600,
    "year": "2025",
    "title": "How to shoot star trails with your smartphone",
    "type": "astronomy-post",
    "content": "# How to shoot star trails with your smartphone\n\nThis complete guide covers everything you need to know to capture beautiful star trail photos using just your smartphone.\n\n**Date:** October 17, 2025\n**Tags:** Astronomy, Star trails, Timelapse, Sky Tonight, Astrophotography\n\n---\n\n## Introduction\n\nIn this guide, I'll show you how to photograph [circumpolar star trails](https://en.wikipedia.org/wiki/Circumpolar_star) with your smartphone. The image I used as the thumbnail for this article was taken with my phone under a [Bortle level](https://en.wikipedia.org/wiki/Bortle_scale) 3 sky, just before sunrise.\n\nThe Bortle scale is a nine-level scale used by astronomers to describe the brightness of the night sky caused by [light pollution](https://en.wikipedia.org/wiki/Light_pollution).\n\n- **A Bortle 1** sky is extremely dark.\n- **A Bortle 9** sky is bright and blocks most starlight.\n\nThe image below shows how much `light pollution` affects what you can see in the night sky, from a dark `Bortle 1` location (left) to a bright `Bortle 9` city sky (right).\n\nBy following this guide, you'll learn how to:\n\n- Locate the [north star (Polaris)](https://en.wikipedia.org/wiki/Polaris) or the [south star (Sigma Octantis)](https://en.wikipedia.org/wiki/Sigma_Octantis) to find the center of rotation in the night sky.\n- Use your phone's `pro mode` to take long-exposure photos.\n- Automatically capture hundreds of photos throughout the night.\n- Combine those images into a single image to create star trails.\n- Create a [time-lapse](https://en.wikipedia.org/wiki/Time-lapse_photography) video from the same photos (Bonus).\n\nEverything in this article is explained step-by-step, so even if you've never tried astrophotography before, you'll be able to follow along easily.\n\n---\n\n## Star Trails\n\nStar trails are photographs that show the motion of stars across the night sky, which is caused by the Earth's rotation. Capturing a star trail is one of the best ways to get started with [astrophotography](https://en.wikipedia.org/wiki/Astrophotography).\n\nThe look of your final image depends entirely on where you point your camera. Each direction creates a different effect:\n\n- **Pointing north or south:** This creates a circular pattern around the celestial poles (Polaris in the north, Sigma Octantis in the south).\n- **Pointing east:** You will capture stars as long arcs rising from the horizon.\n- **Pointing west:** You will capture stars as long arcs setting toward the horizon.\n\n### Northern Hemisphere\n\nIf you live in the [northern hemisphere](https://en.wikipedia.org/wiki/Northern_Hemisphere), you can see the star **Polaris**. The Earth's [north celestial pole](https://en.wikipedia.org/wiki/Celestial_pole#Finding_the_north_celestial_pole) is almost perfectly aligned with it, which means **Polaris remains visible all year round** and never sets below the horizon.\n\nStars that never set below the horizon are called [circumpolar stars](https://en.wikipedia.org/wiki/Circumpolar_star). When you take a photo with Polaris in the center, you capture these circumpolar stars creating perfect circles around it, while other stars form arcs across the sky.\n\n### Southern Hemisphere\n\nIf you live in the [southern hemisphere](https://en.wikipedia.org/wiki/Celestial_pole#Finding_the_south_celestial_pole), try to locate the star **Sigma Octantis**. If you've never heard of it before, don't worry, I'll show you how to use a planetarium app to help you find any object in the sky.\n\n---\n\n## Required Equipment\n\nTo capture a star trail, **you take hundreds of short-exposure images over several hours**. Later, you will combine those images using photo-editing software to create the final star trail effect.\n\nHere is what you will need:\n\n- A tripod.\n- Your smartphone.\n- An app that can automatically tap the shutter button.\n- A planetarium app to help find Polaris or Sigma Octantis.\n\n### Smartphone\n\nBoth Android and iPhone users can follow this guide. What matters most is that your camera app allows you to shoot in **pro mode**.\n\nPro mode lets you change the camera settings manually. This is important because every photo needs to have the same settings throughout the night.\n\nIf you don't use pro mode, your phone will keep changing the camera settings on its own while shooting, which will ruin the final image.\n\nIn the steps below, I will show you how to find pro mode on your phone. I am using an Android device, but iPhone users should be able to find similar options in their camera app.\n\nLaunch your camera app and tap the pro mode option.\n\nInside pro mode, you will need to adjust three main settings:\n\n- **ISO:** Controls how sensitive your camera sensor is to light.\n- **Shutter speed:** How long the camera's shutter stays open.\n- **Focus:** Adjusts the lens to make a specific area in your frame appear sharp and clear.\n\nFound them? Great! I'll explain how to set them later in this guide.\n\n### Planetarium App\n\nA planetarium app lets you see the sky directly on your phone. The app uses your location and your phone's compass to accurately show celestial objects.\n\nThis guide is written for people in the `northern hemisphere`, where Polaris is visible and easy to find.\n\nIf you're in the `southern hemisphere`, Polaris won't be visible. Instead, look for `Sigma Octantis`, the South Star, which is much fainter and harder to spot.\n\nThere are many free apps that work on both Android and iPhone. I use [Sky Tonight](https://vitotechnology.com/apps/sky-tonight) every day, it's simple and works well. Give it a try.\n\nOpen the app and go through the setup. Once done, click on the `compass icon` in the lower-right corner, or just move your phone around. This helps the app match your view to the real sky.\n\nKeep moving your phone until you see the `south` direction (in the southern hemisphere) or `north` (in the northern hemisphere).\n\nWhen you're pointing north, Polaris should be highlighted by the app.\n\nIf it's not, tap the **search button**.\n\nType `Polaris`, then press the locate icon.\n\nNow you should see Polaris highlighted. Move your phone slowly until the star is in the center of the screen.\n\n### Automatic Clicker App\n\nSince you'll be taking hundreds of photos, you need an app that can take them automatically for you. The built-in camera apps on most phones don't have this feature.\n\nOn Android, I use [Auto Clicker-Automatic tap](https://play.google.com/store/apps/details?id=com.truedevelopersstudio.automatictap.autoclicker&hl=en-US). It is free, easy to use, and works well. Unfortunately, it's not available for iPhone, but you can find similar apps in the App Store.\n\nOpen the app and enable `single target mode`, since we only need to tap one thing, the shutter button.\n\nMake sure the control panel is showing on screen.\n\nYou can disable this mode now, I will show you how to use it properly later.\n\n### Tripod\n\nTo get a clean star trail, your phone must stay completely still during the entire shoot. That's why a tripod is important.\n\nAny tripod that can hold your phone will do. No need to spend a lot, even a cheap one works fine.\n\nIf you don't have a tripod, you can lean your phone against a wall or somewhere solid that won't move. I have done this many times when I couldn't afford a tripod :)\n\n---\n\n## Collecting Data\n\nPlace your smartphone on the tripod. Then, open the planetarium app you downloaded. Search for the star Polaris and follow it until you find it. Try to center it in the screen, or put it in a corner if you prefer.\n\nMake sure the phone stays still all night. If it is windy, cover the phone gently, but don't block the camera lens.\n\nNow, open the auto clicker app and turn on `single target mode`.\n\nExit the auto clicker, then open the camera app. Switch to `pro mode`, and place the auto clicker's floating blue circle over the shutter button.\n\nNow, it is time to adjust the settings. First, tap on `AF` and set it to **infinity**. This locks the camera focus on distant stars. If you skip this, the camera will try to focus on something nearby, and your photos will be blurry.\n\nNow, tap `S` (it might say something different depending on your app) to change the shutter speed. Set it to 10 seconds. That means the camera stays open for 10 seconds each time, gathering as much light as possible.\n\nDon't go over 20 seconds, especially if you're using a tripod. If you set it too high, stars will show up as streaks, not circles. Try taking a test shot at 30 seconds just to see what happens. You'll see that the stars are all stretched.\n\nSo stick to 10â€“20 seconds. For now, use 10.\n\nNext, tap the `ISO` button and set it between 800 and 1600. If you're in a city with light pollution, keep it at 800 or less. Again, don't push the ISO too high, like 25600, unless you want to hurt your eyes later when looking at the photo ðŸ˜…\n\nNow, set the timer to 3 or 5 seconds. This helps reduce blur caused by shaking the phone when pressing the shutter. The timer lets the phone settle before taking the shot.\n\nI said it's rare to find an intervalometer built into the camera app. But in my case, it's there. Not sure if all Android phones have it.\n\nIf your app has the `Interval` option, set it to 1 or 2 seconds. That means: after a 10-second exposure, the camera waits 1 or 2 seconds before taking the next one.\n\nIf your camera app doesn't have this, no problem, we'll use the auto clicker instead. But first, let's take a test shot. Make sure your camera settings are as follows:\n\n- **ISO:** 1200.\n- **Timer:** 3 or 5 seconds.\n- **Shutter speed:** 10 seconds.\n- **Focus:** Infinity (max number).\n\nTake a photo. Open it and check if the stars show up. Here's what I got:\n\nZoom in on a star. Is it a clean circle? If not, check your focus or increase the timer. If the image is too\ndark, increase the ISO a little. Take another photo, look at it, adjust, repeat. Keep doing this until you're\nhappy with the result.\n\nOnce you have found the settings you like, delete the earlier test shots. Now you're ready to let the phone run\non its own and collect data.\n\nMake sure the phone is fully charged before starting. Then, disable Wi-Fi, turn on airplane mode, and set the\nphone to silent. This helps prevent interruption, like incoming calls, that could stop the photo sequence.\n\nPut the auto clicker's floating circle on the shutter speed again, and open the settings.\n\nSet the **Stop after** to `Run indefinitely`. Then calculate the interval like this:\n\n```text\nInterval = shutter speed + timer + 1 second\nInterval = 10 + 3 + 1\nInterval = 14 seconds\n```\n\nThe extra second gives your phone time to save the photo and prepare for the next one. If you want to gain a few\nseconds, decrease the timer as much as you can. Once done, click Save.\n\nNow tap the blue icon to start the auto clicker. Don't touch the phone. Leave it alone for 3 to 5 hours.\n\nYou're done waiting? Good. Tap the same icon again, it is now a square, to stop the auto clicker. If you think\nthe last photo was affected by you touching the phone, go ahead and delete it.\n\n---\n\n## Processing the Data\n\n### Windows & macOS\n\nYou are now ready to process the data you collected. For Windows & macOS users, download the [StarStax program](https://markus-enzweiler.de/software/starstax/). Then connect your phone to your computer with a USB cable and transfer your photos.\n\nCreate a folder named `StarTrails` to stay organized. Inside it, create another folder called `images`, and move only your `jpg` files there.\n\nNow, open `StarStax`. Drag and drop all your images where it says \"Drop Images Here.\"\n\nOnce the images are loaded, click on a file name to preview it. Every checked image will be used when\nstacking. If you notice any with artifacts, simply uncheck them before continuing.\n\nNext, click the `Preferences` button in the top-right corner to open the blending\nsettings.\n\nBy default, `Lighten` mode is selected, this is what we want. You can also select\n`Gap filling` to smooth out any breaks in the star trails. Keep\n`Lighten` active, then press `Ctrl+P` or click\n`Start processing` in the top-left corner to begin stacking.\n\nTo save your final image, click `Save as` next to the\n`Start processing` button or press `Ctrl+S`.\n\nInside your `StarTrails` folder, create another folder called\n`StackedImage` and save the result there.\n\nBefore saving, make sure to add the correct file extension. If you just want the final image, use\n`.png` or `.jpg`. If you plan to edit it further, for example, to\nremove satellites or add effects, save it as `.tiff`.\n\n### Linux\n\nThere's an excellent open-source project on GitHub called [startrails](https://github.com/gkyle/startrails). It lets you stack images, remove\nartifacts, and fill gaps in your star trails. It's completely free and open source.\n\nOn GitHub, click on the `Code` button and download the project as a ZIP file.\n\nUnzip the file and navigate to the project folder using these commands:\n\n```bash\ncd Downloads\ncd startrails\n```\n\nTo start the program, run this script:\n\n```bash\n./run.sh\n```\n\nNow wait, the script will create a virtual environment using the\n[uv package manager](https://docs.astral.sh/uv/). If you have an **NVIDIA GPU**,\nthe script will automatically detect it and install the required dependencies to use your GPU for faster\nprocessing.\n\nAt the end of the installation, you should see this window:\n\nClick on the `Add star frames` button and import your images.\n\nAfter loading the images, you will be able to preview the images in the app.\n\nNext, click `Stack images` to start the stacking process. In the options, set\n`Fade Frames` to None. If you have a GPU, enable it and adjust the\n`Batch Size` based on your GPU's available VRAM.\n\nHigher batch sizes will speed up processing but also use more memory. Click `OK` to start\nthe stacking process.\n\nThe final image will be saved in the `projects` folder:\n\n```bash\ncd Downloads/startrails/projects/\ntree default.project/\n```\n\nOutput:\n\n```text\ndefault.project/\nâ”œâ”€â”€ stacked-IMG_20240818_035232-2025-10-14-22-40.jpg\nâ””â”€â”€ stacked-IMG_20240818_035232-2025-10-14-22-47.jpg\n```\n\nTo fill gaps in the star trails, click the `Fill gaps` button. The result will be saved in\nthe same folder.\n\nYou can also remove satellite streaks from the images by clicking on the\n`Detect streaks` button. This uses a deep learning model to find and remove streaks\nautomatically.\n\nClick `Stack images` again. Choose `Remove` in the streaks section and\nset `Fade frames` to None and click `OK`. Your new image will now be\nfree of satellite trails.\n\n---\n\n## Bonus: Time-Lapse\n\nHere is a bonus for you, with the same data, you can create a time-lapse like this one.\n\nTo do this, you'll need video editing software to combine your images into a video. I use\n[Shotcut](https://www.shotcut.org/), but feel free to use any other program you like.\n\n### StarStax\n\nIn StarStax, enable the `Save after each step` option. Enter a name in the\n`image name` field, this will be used as a prefix for each frame. Then click\n`Browse` to select where the images should be saved.\n\nCreate a new folder inside `StarTrails` and call it `Timelapse`.\nInside it, create another folder named `Images`.\n\nSelect the `Images` folder when clicking on `Browse`.\n\nIf you downloaded [Shotcut](https://www.shotcut.org/), follow these steps.\nOtherwise, try to reproduce them in your own editing software.\n\nOpen Shotcut and go to `Settings` â†’ `Video mode` â†’\n`Custom` â†’ `Add` to create a new video mode.\n\nSet the resolution and aspect ratio based on your phone camera's resolution. Set the frame rate to\n**30 or 60 FPS**, then click **OK**.\n\nDrag your first image (for example, `image_name_00000000`) into Shotcut. Then open the\n`Properties` panel and check `Image sequence`, this tells Shotcut to\ncombine all the images into a single video.\n\nPress the `+` button or the `A` key to add the video to the track.\n\nClick `Export` â†’ `Advanced`, and make sure the format is\n`MP4`. Check that the resolution and FPS match your project settings. Finally, click\n`Export file` to create your time-lapse.\n\n---\n\n## Conclusion\n\nIn this guide, you learned how to use **pro mode** on your camera app to control every setting manually.\n\nYou used an **auto clicker** to capture images automatically throughout the night, and learned how to use\ncomputer programs to **stack** those images into a single star trail, and even how to turn them into a\n**time-lapse** video.\n\nEverything you've learned here also applies to digital cameras.\n\nHappy stargazing\n",
    "view_count": 7,
    "read_count": 2,
    "claps_count": 0
  },
  {
    "id": "65b4926d-aade-4e5f-846d-aa07440f4b4c",
    "name": "elasticsearch-pre-filtering-with-knn-search",
    "tags": ["Elasticsearch", "kNN", "Semantic search"],
    "creation_date": 1754953200,
    "year": "2025",
    "title": "Pre-filtering with kNN search in Elasticsearch",
    "type": "blog-post",
    "content": "# Pre-filtering with kNN search in Elasticsearch\n\nHow to apply filters to an index to remove documents that donâ€™t meet certain requirements before using kNN search.\n\n**Date:** August 12, 2025\n**Tags:** Elasticsearch, kNN, Semantic search\n\n-----\n\n## Introduction\n\nIn this article, I will show you how to use pre-filtering with kNN search. For a more detailed walkthrough, check out the video tutorial:\n\nhttps://www.youtube.com/watch?v=ESC-ome_Q1o\n\nYou can also find all related notebooks and slides in my [GitHub repository](https://github.com/ImadSaddik/ElasticSearch_Python_Course).\n\n-----\n\n## Pre-filtering\n\n**Pre-filtering** means we apply filters to an index before doing anything else. For example, we can filter out documents that don't meet certain requirements before using kNN search.\n\n### Preparing the index\n\nSince we're using kNN search, we need to set the data type for the embedding field to `dense_vector`. Elasticsearch doesn't do this by itself like it does for other fields, so we have to do it manually.\n\n```python\nfrom elasticsearch import Elasticsearch\n\nes = Elasticsearch(\"http://localhost:9200\")\nclient_info = es.info()\nprint(\"Connected to Elasticsearch!\")\n\nes.indices.delete(index=\"apod\", ignore_unavailable=True)\nes.indices.create(\n    index=\"apod\",\n    mappings={\n        \"properties\": {\n            \"embedding\": {\n                \"type\": \"dense_vector\",\n            }\n        }\n    },\n)\n```\n\n### Indexing documents\n\nLet's use the `Apod` dataset, you can find it in [this GitHub repository](https://github.com/ImadSaddik/ElasticSearch_Python_Course/blob/main/data/apod.json). Start by reading the file.\n\n```python\nimport json\n\nwith open(\"../data/apod.json\") as f:\n    documents = json.load(f)\n```\n\nThen, let's use an embedding model from Hugging Face. An embedding model converts text into a dense vector. I will use [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) in this tutorial; it is a small model that should work fine if you don't have a GPU.\n\nFirst, make sure to install the `sentence_transformers` library in your python environment.\n\n```bash\npip install sentence-transformers\n```\n\nThen, download the model from Hugging Face and pass it to the device. The code will automatically detect if you have a GPU or not.\n\n```python\nimport torch\nfrom sentence_transformers import SentenceTransformer\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\nmodel = model.to(device)\n```\n\nLet's use the model to embed the `explanation` field for all documents. We will also use the `bulk API` to index the documents in the `apod index`.\n\n```python\nfrom tqdm import tqdm\n\ndef get_embedding(text):\n    return model.encode(text)\n\noperations = []\nfor document in tqdm(documents, total=len(documents), desc=\"Indexing documents\"):\n    year = document[\"date\"].split(\"-\")[0]\n    document[\"year\"] = int(year)\n\n    operations.append({\"index\": {\"_index\": \"apod\"}})\n    operations.append(\n        {\n            **document,\n            \"embedding\": get_embedding(document[\"explanation\"]),\n        }\n    )\n\nresponse = es.bulk(operations=operations)\n```\n\nIf the indexing is successful, you should see `response[\"errors\"]` as `False`.\n\n### Pre-filtering & kNN search\n\nBefore using pre-filtering, let's use kNN search only. The following search query will use kNN search to find 10 documents that are very similar to the query.\n\n```python\nquery = \"What is a black hole?\"\nembedded_query = get_embedding(query)\n\nresult = es.search(\n    index=\"apod\",\n    knn={\n        \"field\": \"embedding\",\n        \"query_vector\": embedded_query,\n        \"num_candidates\": 20,\n        \"k\": 10,\n    },\n)\n\nnumber_of_documents = result.body[\"hits\"][\"total\"][\"value\"]\nprint(f\"Found {number_of_documents} documents\")\n# Found 10 documents\n```\n\nHere we got 10 documents that are very similar to the query. Let's print the first 3 documents.\n\n```python\nfor hit in result.body[\"hits\"][\"hits\"][:3]:\n    print(f\"Score: {hit['_score']}\")\n    print(f\"Title: {hit['_source']['title']}\")\n    print(f\"Explanation: {hit['_source']['explanation']}\")\n    print(\"-\" * 80)\n```\n\n```text\nScore: 0.80657506\nTitle: Black Hole Accreting with Jet\nExplanation: What happens when a black hole devours a star? Many details remain unknown, but observations are providing new clues. In 2014, a powerful explosion was recorded by the ground-based robotic telescopes of the All Sky Automated Survey for SuperNovae (Project ASAS-SN), with followed-up observations by instruments including NASA's Earth-orbiting Swift satellite. Computer modeling of these emissions fit a star being ripped apart by a distant supermassive black hole. The results of such a collision are portrayed in the featured artistic illustration. The black hole itself is a depicted as a tiny black dot in the center. As matter falls toward the hole, it collides with other matter and heats up. Surrounding the black hole is an accretion disk of hot matter that used to be the star, with a jet emanating from the black hole's spin axis.\n--------------------------------------------------------------------------------\nScore: 0.80611444\nTitle: Black Hole Accreting with Jet\nExplanation: What happens when a black hole devours a star? Many details remain unknown, but recent observations are providing new clues. In 2014, a powerful explosion was recorded by the ground-based robotic telescopes of the All Sky Automated Survey for SuperNovae (ASAS-SN) project, and followed up by instruments including NASA's Earth-orbiting Swift satellite. Computer modeling of these emissions fit a star being ripped apart by a distant supermassive black hole. The results of such a collision are portrayed in the featured artistic illustration. The black hole itself is a depicted as a tiny black dot in the center. As matter falls toward the hole, it collides with other matter and heats up. Surrounding the black hole is an accretion disk of hot matter that used to be the star, with a jet emanating from the black hole's spin axis.\n--------------------------------------------------------------------------------\nScore: 0.77729917\nTitle: First Horizon Scale Image of a Black Hole\nExplanation: What does a black hole look like? To find out, radio telescopes from around the Earth coordinated observations of black holes with the largest known event horizons on the sky. Alone, black holes are just black, but these monster attractors are known to be surrounded by glowing gas. The first image was released yesterday and resolved the area around the black hole at the center of galaxy M87 on a scale below that expected for its event horizon. Pictured, the dark central region is not the event horizon, but rather the black hole's shadow -- the central region of emitting gas darkened by the central black hole's gravity. The size and shape of the shadow is determined by bright gas near the event horizon, by strong gravitational lensing deflections, and by the black hole's spin. In resolving this black hole's shadow, the Event Horizon Telescope (EHT) bolstered evidence that Einstein's gravity works even in extreme regions, and gave clear evidence that M87 has a central spinning black hole of about 6 billion solar masses. The EHT is not done -- future observations will be geared toward even higher resolution, better tracking of variability, and exploring the immediate vicinity of the black hole in the center of our Milky Way Galaxy.\n--------------------------------------------------------------------------------\n```\n\nLet's look at the years of the documents returned by the regular kNN search.\n\n```python\nfor hit in result.body[\"hits\"][\"hits\"]:\n    print(f\"Year: {hit['_source']['year']}\")\n```\n\nWe can see that the years are different.\n\n```text\nYear: 2024\nYear: 2017\nYear: 2019\nYear: 2022\nYear: 2018\nYear: 2020\nYear: 2024\nYear: 2024\nYear: 2022\nYear: 2020\n```\n\nLet's run the same query, but this time we will use **pre-filtering** to filter the documents based on the year. Let's say we want to filter the documents to only include those from the year 2024.\n\nWe do this by adding a `filter` clause to the kNN query. The `filter` clause is a regular query that filters the documents before the kNN search is performed.\n\n```python\nquery = \"What is a black hole?\"\nembedded_query = get_embedding(query)\n\nresult = es.search(\n    index=\"apod\",\n    knn={\n        \"field\": \"embedding\",\n        \"query_vector\": embedded_query,\n        \"num_candidates\": 20,\n        \"k\": 10,\n        \"filter\": {\"term\": {\"year\": 2024}},\n    },\n)\n\nnumber_of_documents = result.body[\"hits\"][\"total\"][\"value\"]\nprint(f\"Found {number_of_documents} documents\")\n# Found 10 documents\n```\n\nWe still get 10 documents. Let's look at the year field in each document.\n\n```python\nfor hit in result.body[\"hits\"][\"hits\"]:\n    print(f\"Year: {hit['_source']['year']}\")\n```\n\nAs you can see, the documents returned are only from the year 2024.\n\n```text\nYear: 2024\nYear: 2024\nYear: 2024\nYear: 2024\nYear: 2024\nYear: 2024\nYear: 2024\nYear: 2024\nYear: 2024\nYear: 2024\n```\n\nLet's look at the first 3 documents returned by the kNN search to confirm that they are similar to the query.\n\n```python\nfor hit in result.body[\"hits\"][\"hits\"][:3]:\n    print(f\"Score: {hit['_score']}\")\n    print(f\"Title: {hit['_source']['title']}\")\n    print(f\"Explanation: {hit['_source']['explanation']}\")\n    print(\"-\" * 80)\n```\n\n```text\nScore: 0.80657506\nTitle: Black Hole Accreting with Jet\nExplanation: What happens when a black hole devours a star? Many details remain unknown, but observations are providing new clues. In 2014, a powerful explosion was recorded by the ground-based robotic telescopes of the All Sky Automated Survey for SuperNovae (Project ASAS-SN), with followed-up observations by instruments including NASA's Earth-orbiting Swift satellite. Computer modeling of these emissions fit a star being ripped apart by a distant supermassive black hole. The results of such a collision are portrayed in the featured artistic illustration. The black hole itself is a depicted as a tiny black dot in the center. As matter falls toward the hole, it collides with other matter and heats up. Surrounding the black hole is an accretion disk of hot matter that used to be the star, with a jet emanating from the black hole's spin axis.\n--------------------------------------------------------------------------------\nScore: 0.75311136\nTitle: A Black Hole Disrupts a Passing Star\nExplanation: What happens to a star that goes near a black hole? If the star directly impacts a massive black hole, then the star falls in completely -- and everything vanishes. More likely, though, the star goes close enough to have the black hole's gravity pull away its outer layers, or disrupt, the star. Then, most of the star's gas does not fall into the black hole. These stellar tidal disruption events can be as bright as a supernova, and an increasing amount of them are being discovered by automated sky surveys. In the featured artist's illustration, a star has just passed a massive black hole and sheds gas that continues to orbit. The inner edge of a disk of gas and dust surrounding the black hole is heated by the disruption event and may glow long after the star is gone.\n--------------------------------------------------------------------------------\nScore: 0.7479558\nTitle: Swirling Magnetic Field around Our Galaxy's Central Black Hole\nExplanation: What's happening to the big black hole in the center of our galaxy? It is sucking in matter from a swirling disk -- a disk that is magnetized, it has now been confirmed. Specifically, the black hole's accretion disk has recently been seen to emit polarized light, radiation frequently associated with a magnetized source. Pictured here is a close-up of Sgr A*, our Galaxy's central black hole, taken by radio telescopes around the world participating in the Event Horizon Telescope (EHT) Collaboration. Superposed are illustrative curved lines indicating polarized light likely emitted from swirling magnetized gas that will soon fall into the 4+ million solar mass central black hole. The central part of this image is likely dark because little light-emitting gas is visible between us and the dark event horizon of the black hole. Continued EHT monitoring of this and M87's central black hole may yield new clues about the gravity of black holes and how infalling matter creates disks and jets.\n--------------------------------------------------------------------------------\n```\n\n-----\n\n## Conclusion\n\nWe've reached the end of this journey! I hope you had fun and learned a lot. I also hope I did a good job teaching you how to use Elasticsearch. It's a great tool that I really enjoy. Best of luck on your learning journey, and happy coding!\n",
    "view_count": 136,
    "read_count": 2,
    "claps_count": 0
  },
  {
    "id": "86ddf678-9319-4c20-96ee-4be9c565cedd",
    "name": "free-planetarium-apps",
    "tags": [
      "Astronomy",
      "Planetarium",
      "Stellarium",
      "Sky Tonight",
      "SkySafari"
    ],
    "creation_date": 1758322800,
    "year": "2025",
    "title": "Learn the night sky with free planetarium applications",
    "type": "astronomy-post",
    "content": "# Learn the night sky with free planetarium applications\n\nA list of free planetarium applications to explore the night sky on your computer or mobile device.\n\n**Date:** September 20, 2025  \n**Tags:** Astronomy, Planetarium, Stellarium, Sky Tonight, SkySafari\n\n-----\n\n## Introduction\n\n[Planetarium](https://en.wikipedia.org/wiki/Planetarium_software) applications helped me learn a lot about the universe. I read stories about [constellations](https://en.wikipedia.org/wiki/Constellation), stars, and planets. I memorized the names of the brightest stars. I learned how to read astronomical data like [right ascension](https://en.wikipedia.org/wiki/Right_ascension), [declination](https://en.wikipedia.org/wiki/Declination), [magnitude](https://en.wikipedia.org/wiki/Magnitude_(astronomy)), [star type](https://en.wikipedia.org/wiki/Stellar_classification), etc.\n\nI use planetarium applications on both my phone and my computer. Some are free, others are paid. Some are simple, others are complex. Overall, I believe everyone should have a planetarium app at hand. In this article, I will share with you the free applications that I have used over the past years.\n\n-----\n\n## Computer applications\n\n### Stellarium\n\n*A view of the Milky Way as seen from Stellarium.*\n\n[Stellarium](https://stellarium.org/) was the first planetarium application I used. I installed it on my Ubuntu computer after discovering that it is free and open source. I donâ€™t remember exactly how I found it, but I think it was through a YouTube video about astronomy.\n\nStellarium has a simple, intuitive user interface and is packed with features. It is my favorite application, and Iâ€™d like to share some of the features I really enjoy.\n\n#### Object information\n\nClicking on any object, whether it is a star, planet, galaxy, or something else, displays a lot of information about it. You can see details such as its magnitude, distance, position in the night sky, and many other useful facts.\n\n*Clicking on an object in Stellarium reveals detailed information about it.*\n\n#### What's up today\n\nThe `What's up today` feature shows you a list of interesting celestial events happening on the current day, such as planets, galaxies, and comets. It's a great way to discover new objects to observe in the night sky.\n\nTo access this feature, click on the `Astronomical calculations window` icon on the left toolbar or press `F10`, then go to the `What's up today (WUT)` tab.\n\nWithin the `WUT` tab, you can filter events by category (for example, planets or comets) and by the time of day (such as any time during the night or just in the evening). You can also filter the results by magnitude, which is useful if you want to see only the brightest objects.\n\nThe results appear in the `Matching objects` table. You can double-click any row to center the view on that object.\n\n*The **What's up today** feature in Stellarium helps you discover interesting celestial events.*\n\n#### Phenomena\n\nThe `Phenomena` feature allows you to calculate when certain celestial events will occur between two objects. By default, the tool calculates only [conjunctions](https://en.wikipedia.org/wiki/Conjunction_(astronomy)), but you can also select [oppositions](https://en.wikipedia.org/wiki/Opposition_(astronomy)), [elongations](https://en.wikipedia.org/wiki/Elongation_(astronomy)), [quadratures](https://en.wikipedia.org/wiki/Quadrature_(astronomy)), [perihelion](https://simple.wikipedia.org/wiki/Perihelion), and [aphelion](https://simple.wikipedia.org/wiki/Aphelion).\n\nBefore starting the calculation, choose the start and end dates, the two objects, the type of phenomena you want to calculate, and the maximum allowed separation. Then click the `Calculate` button.\n\nDouble-clicking any row in the results table will take you to that event in time and center the objects in the view.\n\n*The **Phenomena** feature in Stellarium helps you find interesting celestial events between two objects.*\n\n#### Eclipses\n\nThe `Eclipses` feature allows you to calculate all types of eclipses, including lunar, solar, and planetary transits. By default, the `All solar eclipses` tab is selected, but you can choose any type of eclipse from the tabs at the bottom. Select the starting year and the number of years you want to search, then click the `Calculate eclipses` button.\n\nAs with the previous features, double-clicking any row in the results table will take you to that event in time and center the relevant objects in the view.\n\n*The **Eclipses** feature in Stellarium helps you find upcoming eclipses.*\n\n#### Image sensor frame\n\nFor astrophotographers, the `Image sensor frame` feature is extremely useful. It allows you to simulate the field of view of your camera and telescope setup, so you can see exactly what portion of the sky will be captured in your images.\n\nYou can select your sensor type, and the tool will display details such as the sensor's dimensions, [binning](https://en.wikipedia.org/wiki/Pixel_binning), X and Y scale, and rotation angle. You can also choose your telescope and lens configuration, including [focal reducers](https://en.wikipedia.org/wiki/Telecompressor) or [extenders](https://en.wikipedia.org/wiki/Extension_tube), and see the resulting field of view and multiplicity.\n\nThis makes it easy to plan your [astrophotography](https://en.wikipedia.org/wiki/Astrophotography) sessions and ensure your target fits perfectly within your camera's frame.\n\n*The **Image sensor frame** feature in Stellarium helps you visualize your camera and telescope setup.*\n\n#### Sky culture\n\nStellarium supports a variety of [sky cultures](https://en.wikipedia.org/wiki/Cultural_astronomy), allowing you to view the night sky as interpreted by different cultures around the world. You can choose from a wide range of cultural sky representations.\n\nTo explore these options, click on the `Sky and viewing options window` icon on the left toolbar or press `F4`, then go to the `Sky culture` tab.\n\n*On the left is the modern sky culture, and on the right is the Arabic sky culture.*\n\n#### Simulating sky conditions\n\nIn the past, [people enjoyed the night sky](https://harmoniouscosmos.com/ancient-astronomies-and-the-human-fascination-with-the-sky/) without the effects of [light pollution](https://en.wikipedia.org/wiki/Light_pollution). Today, in most cities, it is difficult to see more than a few stars due to artificial lights, which means we miss out on much of the skyâ€™s beauty.\n\nStellarium allows you to simulate different sky conditions, such as light pollution, and atmospheric effects. This is especially useful for planning observations based on your local environment.\n\nTo adjust these settings, click on the `Sky and viewing options window` on the left toolbar or press `F4`, then go to the `Sky` tab.\n\n*The **Sky and viewing options window** in Stellarium allows you to customize various sky settings.*\n\nTo control whether the atmosphere is rendered, check the `Atmosphere visualization` checkbox. For more options, click the wrench icon next to it to adjust temperature, pressure, and other parameters.\n\n*Comparison between a sky with atmosphere (left) and without atmosphere (right).*\n\nTo simulate light pollution, use the slider in the `light pollution` section. This slider lets you change the [Bortle scale](https://en.wikipedia.org/wiki/Bortle_scale) from 1 (excellent dark sky) to 9 (inner city sky). Move the slider to see how the sky changes under different conditions.\n\n*Comparison between a Bortle 1 sky (left) and a Bortle 9 sky (right).*\n\nYou can also adjust other settings, such as making stars twinkle, changing the [sky projection](https://en.wikipedia.org/wiki/General_Perspective_projection), and modifying the [field of view (FOV)](https://en.wikipedia.org/wiki/Field_of_view).\n\n#### Plugins\n\nBecause Stellarium is open source, it offers a wide range of plugins that add extra features to the application. You can find plugins that add [quasars](https://en.wikipedia.org/wiki/Quasar), [pulsars](https://en.wikipedia.org/wiki/Pulsar), simulate periodic [meteor showers](https://en.wikipedia.org/wiki/Meteor_shower), and much more.\n\nTo explore plugins, click on the `Configuration window` icon on the left toolbar or press `F2`, then go to the `Plugins` tab. Select a plugin and check the `Load at startup` box to enable it. Some plugins have additional settings you can configure by clicking the `Configure` button.\n\n*The **Plugins** tab in Stellarium allows you to manage and configure various plugins.*\n\nFor example, I enabled a plugin that measures [angular distances](https://en.wikipedia.org/wiki/Angular_distance) between objects in the sky. After restarting Stellarium, I could use this plugin by clicking on the `angular distance measurement tool` on the bottom toolbar.\n\n*The **angular distance measurement** plugin in Stellarium helps you measure distances between celestial objects.*\n\n#### Full guide\n\nI could write about Stellarium all day, but I will stop here. If you want to learn more about Stellarium and its features, I recommend checking out the [official Stellarium guide](https://stellarium.org/files/guide.pdf). It is a complete resource that covers all aspects of the application.\n\n-----\n\n## Mobile applications\n\nI have used many planetarium applications on my phone over the years. Currently, I only use `Sky Tonight` and `SkySafari Legacy`. Unfortunately, I can't discuss SkySafari Legacy because it is no longer available for download. The other SkySafari versions are paid, and in this article, I only cover free applications that I use.\n\n### Sky Tonight\n\n*A view of the Scorpius constellation as seen from Sky Tonight.*\n\n`Sky Tonight` is a free planetarium application developed by [Vito Technology](https://vitotechnology.com/) and is available on both [Android](https://play.google.com/store/apps/details?id=com.vitotechnology.sky.tonight.map.star.walk&referrer=utm_source%3Dvitosite%26utm_medium%3Dsky-tonight%26utm_campaign%3Dproduct) and [iOS](https://apps.apple.com/us/app/sky-tonight-stargazing-guide/id1570594940). Vito Technology has also developed other applications, such as [Star Walk 2](https://starwalk.space/en), which I used previously. However, since the launch of Sky Tonight, I have switched to it.\n\nI love the graphics and user interface of Sky Tonight. The application has many features that I will discuss below.\n\n#### Calendar\n\nTo use the calendar feature, tap the `calendar` icon on the main screen.\n\n*The **calendar** button in Sky Tonight opens the calendar view.*\n\nThe calendar displays a list of interesting celestial events happening today, and you can also select any future date to see upcoming events for that day. Unlike Stellarium, you can't filter events by category or magnitude, but you can view all events by selecting a date.\n\n*The calendar in Sky Tonight shows celestial events for the selected date.*\n\nThe calendar also shows [moon phases](https://en.wikipedia.org/wiki/Lunar_phase). When you tap on a date, you'll see detailed information about the moon phase for that day, including age, illumination, angular size, magnitude, rise and set times, and more.\n\n*Detailed information about the moon phase in Sky Tonight.*\n\nThere is a tab for meteor showers you can observe, and another tab that provides information about the sky, such as the length of the day, total darkness start and end times, and more.\n\n*The **meteor showers** and **sky details** tabs in Sky Tonight.*\n\nYou can also use the `share` button to send calendar information to your friends.\n\n*The **share** button in Sky Tonight allows you to share celestial event information.*\n\n#### Stargazing index\n\nTo use this feature, click on the telescope icon at the bottom of the screen in the main view.\n\n*The **Visible Tonight** icon in Sky Tonight.*\n\nThe `Stargazing index` feature gathers information about the start and end of the night, the moon phase, light pollution, and the percentage of cloudiness. All of this data is combined to give you a final score from 0 to 100%. A higher score means better conditions for stargazing.\n\n*The **Stargazing index** in Sky Tonight provides a score based on various factors affecting stargazing conditions.*\n\nYou can adjust the forecast horizon to see how the stargazing index changes over time. This is helpful for planning your stargazing sessions. You can choose between `1 day`, `3 days`, or `7 days`. When you select 3 days or 7 days, additional tabs appear, one for each day.\n\n*You can change the forecast horizon in the **Stargazing index** feature of Sky Tonight.*\n\n#### Stargazing news\n\nThe team behind Sky Tonight regularly publishes articles and quizzes about astronomy. You can find these by clicking the hamburger menu icon, then selecting the `Stargazing news` option.\n\nThey also create beautiful infographics. I really enjoy reading these articles and taking the quizzes.\n\n*The **Stargazing news** section in Sky Tonight provides articles and quizzes about astronomy.*\n\n### Other applications\n\nI have also used other planetarium applications like `Star Walk 2`, `SkyView`, and several others. While I no longer use them, they are still solid options. You can try them out and see which one you like best.\n\n-----\n\n## Conclusion\n\nExploring the night sky is incredibly easy with free planetarium applications. Over time, Iâ€™ve tried many of these applications on both my computer and my phone, and each one has taught me something new about the universe.\n\nIf you have not already, I encourage you to try some of these tools. You might be surprised by how much you can learn, and how much fun you will have just by looking up.\n",
    "view_count": 7,
    "read_count": 2,
    "claps_count": 0
  },
  {
    "id": "ba9e4623-7410-4273-8759-7a20e9cf0f65",
    "name": "python-star-trails",
    "tags": [
      "Python",
      "Astrophotography",
      "Image processing",
      "Timelapse",
      "Star trails"
    ],
    "creation_date": 1761260400,
    "year": "2025",
    "title": "How to make star trails and time-lapses with Python",
    "type": "astronomy-post",
    "content": "# How to make star trails and time-lapses with Python\n\nA guide to creating star trail images and time-lapse videos on any platform using Python.\n\n**Date:** October 24, 2025\n**Tags:** Python, Astrophotography, Image processing, Timelapse, Star trails\n\n---\n\n## Introduction\n\nI created `PyStarTrails` because I needed a simple tool to make star trail images and time-lapse videos on Linux. In this article, I'll show you how to create your own using Python, which works on any platform.\n\nYou will learn the basics of how to **blend photos** together to create a beautiful star trail image. I will also show you how to add cool effects like **comet trails** and **fades**. Finally, we'll use the same code to turn all those images into a time-lapse video, like the one below.\n\n_Time-lapse generated from 408 images at 60 FPS._\n\nThe source code for this project is available on [GitHub](https://github.com/ImadSaddik/PyStarTrails).\n\n---\n\n## Creating your first star trail image\n\n### How it works\n\nYou've captured hundreds of photos, and now it's time to blend them together to create your first star trail image. Each photo can be thought of as having two parts: the stars and the background.\n\nThe background remains still, while the stars appear to move from frame to frame due to Earth's rotation. Our goal is to keep the background consistent while revealing the stars' motion across the sky.\n\nTo do this, we gradually blend the images together to trace the path of each star. The pixels representing the background are mostly dark, while the ones representing the stars are bright.\n\nWe use the [lighten blending mode](https://en.wikipedia.org/wiki/Blend_modes#Lighten_Only) for this process, it takes the brighter value between a pixel in one image and the corresponding pixel in the next. This simple rule creates the illusion of continuous trails.\n\nIf you can't quite visualize it yet, don't worry, I've included some illustrations to show exactly how this blending algorithm works.\n\nTo demonstrate how the `lighten blending mode` works, I created five small `8x8` images. Each cell is colored either black (background) or white (stars).\n\nIn the white cells, I've placed the number `1`, which represents full brightness. In practice, each pixel in an [8-bit image](https://en.wikipedia.org/wiki/8-bit_color) stores a value between `0` and `255`, where `255` corresponds to the maximum brightness. So, in this simplified example, `1` stands for `255`.\n\nThe black cells correspond to a brightness value of `0`. For clarity, they are left empty in the diagram because they are greater in number than the white cells.\n\n_A sequence of five 8x8 grids arranged in chronological order._\n\nIn the first image, there are three stars. From one frame to the next, each star moves one pixel to the right and one pixel down. By the final frame, only one star remains visible, as the other two have moved outside the `8x8` grid.\n\nNow, let's apply the `lighten blending mode` to the first two images. In the illustration, you'll see them represented as inputs to the `max()` function. This function compares the pixel values from both images and keeps the brighter one for each position. The resulting image is a blend of the two, showing all the stars that were visible in either frame.\n\n_Blending the first two images with the max function._\n\nThe blended image becomes the new input for the next iteration of the `max()` function. The third image is then used as the second argument. Blend these two together, and repeat the process with the remaining images until you reach the fifth one.\n\nThe final blended result is your complete star trail image, showing the continuous paths traced by the stars over time.\n\n_Iteratively applying the max function to combine the brightest pixels from each frame._\n\n### Putting it into practice\n\nBefore running the script, make sure you have the necessary libraries installed:\n\n```bash\npip install numpy Pillow tqdm\n```\n\nThe following Python script implements the `lighten blending mode` algorithm. Make sure your input images are stored in the `/images/stacking_input` directory and are in either `JPG` or `PNG` format.\n\n```python\nimport glob\nimport os\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom PIL import Image\n\ndef convert_image_to_array(image_path: str) -> np.ndarray:\n    image = Image.open(image_path)\n    return np.array(image)\n\n\ndef get_all_input_images(image_directory: str) -> list[str]:\n    jpg_files = glob.glob(os.path.join(image_directory, \"*.jpg\"))\n    png_files = glob.glob(os.path.join(image_directory, \"*.png\"))\n    image_files = sorted(jpg_files + png_files)\n    return image_files\n\n\ndef save_array_as_image(image_array: np.ndarray, output_path: str) -> None:\n    final_image = Image.fromarray(image_array.astype(np.uint8))\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    final_image.save(output_path)\n\n\nimage_directory = \"./images/stacking_input\"\nimage_files = get_all_input_images(image_directory)\nprint(f\"Found {len(image_files)} image files.\")\n\nif not image_files:\n    raise ValueError(\"No image files found!\")\n\nstacked_array = None\nfor image_file in tqdm(\n    iterable=image_files,\n    total=len(image_files),\n    desc=\"Stacking images to form star trails\",\n):\n    current_array = convert_image_to_array(image_file)\n    if stacked_array is None:\n        stacked_array = current_array\n    else:\n        stacked_array = np.maximum(stacked_array, current_array)\n\noutput_path = \"./output/stacked/stacked_star_trails.jpg\"\nsave_array_as_image(image_array=stacked_array, output_path=output_path)  # type: ignore\nprint(f\"The stacked image has been saved to {output_path}\")\n```\n\nThe code begins by reading all images from the `stacking_input` directory inside the `images` folder.\n\nIt then loops through the images one by one:\n\n- In the first iteration, the first image is simply stored in `stacked_array`, since there's nothing to blend with yet.\n- In the second iteration, the next image is blended with the previous one stored in `stacked_array`. The blending is done using `np.maximum`, which performs the lighten operation **element by element**. In other words, it works **pixel by pixel**.\n- After blending, the result replaces `stacked_array`, and the process continues until all images have been processed.\n\nFinally, the script saves the resulting star trail image to the `output` directory.\n\n_The star trail image that the Python script produced._\n\n---\n\n## Stylizing your star trails\n\n### Creating a comet effect\n\nComets are fascinating objects, they shine with a bright core followed by a long, glowing tail that gradually fades out. We can apply a similar look to our star trails by slightly modifying the way we blend the images.\n\nWith this technique, each star will leave behind a fading trail instead of a continuous, fully bright line.\n\n_Comet at moonrise by [Gabriel Zaparolli](https://www.instagram.com/gabriel_zaparolli/)._\n\nTo create this effect, we introduce a `decay factor` that gradually reduces the brightness of the stars over time. The decay factor controls how long the comet-like tail appears: values close to `1.0` produce longer trails, while values below `0.95` make the trails fade much more quickly.\n\nThe total number of frames also has a strong impact on trail length. Below is a comparison of two results created using the same sequence of images, but with different decay factors:\n\n_**Left:** decay factor = 0.99. **Right:** decay factor = 0.95._\n\nAs you can see, the difference is significant. With a decay factor of `0.99`, the star trails remain visible for much longer than with `0.95`.\n\nThis example uses a stack of `408` images. If we take a pixel with initial brightness `1` and apply the decay factor repeatedly, we can see just how quickly the light fades:\n\n- 1 \\* 0.99^407 = 0.01673108868\n- 1 \\* 0.95^407 = 0.00000000086\n\nWith a decay factor of `0.95`, the pixel brightness becomes very small, and would be even dimmer if we collected more images. This is why choosing the right decay factor is important for control of your star trails.\n\nTo apply the comet effect, we introduce the `decay factor` into the `max()` blending operation. In each iteration, we slightly dim the previously blended image before comparing it with the next frame.\n\n_Applying the decay factor during the first blending step._\n\nOn the next iteration, the updated blended image is multiplied by the `decay factor` again, then compared with the next input image. This process repeats for every frame in the sequence.\n\n_Each new frame adds a bright star position, while older ones gradually fade._\n\nHere is the modified code that applies the comet effect. We introduce a new variable, `decay_factor`, and apply it to the previously blended image before combining it with the next frame using `np.maximum`:\n\n```python\nimport glob\nimport os\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom PIL import Image\n\ndef convert_image_to_array(image_path: str) -> np.ndarray:\n    image = Image.open(image_path)\n    return np.array(image)\n\ndef get_all_input_images(image_directory: str) -> list[str]:\n    jpg_files = glob.glob(os.path.join(image_directory, \"*.jpg\"))\n    png_files = glob.glob(os.path.join(image_directory, \"*.png\"))\n    image_files = sorted(jpg_files + png_files)\n    return image_files\n\ndef save_array_as_image(image_array: np.ndarray, output_path: str) -> None:\n    final_image = Image.fromarray(image_array.astype(np.uint8))\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    final_image.save(output_path)\n\n\nimage_directory = \"./images/stacking_input\"\nimage_files = get_all_input_images(image_directory)\nprint(f\"Found {len(image_files)} image files.\")\n\nif not image_files:\n    raise ValueError(\"No image files found!\")\n\nstacked_array = None\ndecay_factor = 0.99\n\nfor image_file in tqdm(\n    iterable=image_files,\n    total=len(image_files),\n    desc=\"Stacking images to form comet trails\",\n):\n    current_array = convert_image_to_array(image_file).astype(np.float32)\n    if stacked_array is None:\n        stacked_array = current_array\n    else:\n        stacked_array *= decay_factor\n        stacked_array = np.maximum(stacked_array, current_array)\n\noutput_path = \"./output/stacked/stacked_star_trails_comet_style.jpg\"\nsave_array_as_image(image_array=stacked_array, output_path=output_path)  # type: ignore\nprint(f\"The stacked image has been saved to {output_path}\")\n```\n\nHere is the resulting comet-style star trail image:\n\n_Star trails with comet effect applied (decay_factor = 0.99)._\n\n### Adding a fade in and fade out\n\nThe fade-in and fade-out effect creates star trails that gradually brighten toward the middle of the sequence, then fade again toward the end. This keeps the central portion of each trail bright while softening both the beginning and the end.\n\nTo apply this effect, count the total number of images and determine the midpoint.\n\n- Frames **before** the midpoint are gradually brightened, this is the fade-in phase.\n- Frames **after** the midpoint are gradually dimmed, the fade-out phase.\n\n_Image sequence showing the fade-in, midpoint, and fade-out phases._\n\nHere is the updated code that applies the fade-in and fade-out effect. We introduce two new variables:\n\n- `mid_point` determines the center frame in the sequence.\n- `brightness` controls how bright each frame appears based on its position.\n\n_Multiplying each frame by the brightness value for its position._\n\nUnlike the comet effect, we do not apply the `brightness` factor to the blended image. Instead, we multiply it directly with each current image, because the brightness must depend on that frame's position in the sequence (how far it is from the midpoint).\n\n```python\nimport glob\nimport os\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom PIL import Image\n\ndef convert_image_to_array(image_path: str) -> np.ndarray:\n    image = Image.open(image_path)\n    return np.array(image)\n\ndef get_all_input_images(image_directory: str) -> list[str]:\n    jpg_files = glob.glob(os.path.join(image_directory, \"*.jpg\"))\n    png_files = glob.glob(os.path.join(image_directory, \"*.png\"))\n    image_files = sorted(jpg_files + png_files)\n    return image_files\n\ndef save_array_as_image(image_array: np.ndarray, output_path: str) -> None:\n    final_image = Image.fromarray(image_array.astype(np.uint8))\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    final_image.save(output_path)\n\n\nimage_directory = \"./images/stacking_input\"  // Make sure this is set correctly\nimage_files = get_all_input_images(image_directory)\nprint(f\"Found {len(image_files)} image files.\")\n\nif not image_files:\n    raise ValueError(\"No image files found!\")\n\nstacked_array = None\nnumber_of_images = len(image_files)\nmid_point = (number_of_images - 1) / 2.0\n\nfor i, image_file in tqdm(\n    iterable=enumerate(image_files),\n    total=number_of_images,\n    desc=\"Stacking images (fade in/out)\",\n):\n    brightness = 1.0 - abs(i - mid_point) / mid_point if mid_point > 0 else 1.0\n    current_array = convert_image_to_array(image_file).astype(np.float32)\n    modified_array = current_array * brightness\n    if stacked_array is None:\n        stacked_array = modified_array\n    else:\n        stacked_array = np.maximum(stacked_array, modified_array)\n\noutput_path = \"./output/stacked/stacked_star_trails_fade_in_out.jpg\"\nsave_array_as_image(image_array=stacked_array, output_path=output_path)  # type: ignore\nprint(f\"The stacked image has been saved to {output_path}\")\n```\n\nHere is the resulting star trail image with the fade-in and fade-out effect:\n\n_Star trails with a fade-in / fade-out brightness effect._\n\n---\n\n## Creating a star trail time-lapse\n\nInstead of generating just a single star trail image, you can use the same stacking process to create a time-lapse video. The idea is simple: while stacking the images, we save each intermediate blended frame. These saved frames can later be combined into a video.\n\nIn fact, the final star trail image you produced earlier is the last frame in this progression. By capturing every step along the way, you can watch the star trails grow and stretch across the sky over time.\n\nHere's the Python script that does exactly that. It progressively blends each image with the previous result and saves every blended frame to the `output` directory. You can also apply different blending styles (like the comet or fade-in/fade-out effects) by modifying the logic inside the loop.\n\n```python\nimport glob\nimport os\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom PIL import Image\n\ndef convert_image_to_array(image_path: str) -> np.ndarray:\n    image = Image.open(image_path)\n    return np.array(image)\n\ndef get_all_input_images(image_directory: str) -> list[str]:\n    jpg_files = glob.glob(os.path.join(image_directory, \"*.jpg\"))\n    png_files = glob.glob(os.path.join(image_directory, \"*.png\"))\n    image_files = sorted(jpg_files + png_files)\n    return image_files\n\ndef save_array_as_image(image_array: np.ndarray, output_path: str) -> None:\n    final_image = Image.fromarray(image_array.astype(np.uint8))\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    final_image.save(output_path)\n\n\nimage_directory = \"./images/stacking_input\"\nimage_files = get_all_input_images(image_directory)\nprint(f\"Found {len(image_files)} image files.\")\n\nif not image_files:\n    raise ValueError(\"No image files found!\")\n\nstacked_array = None\noutput_directory = \"./output/timelapse/\"\n\nfor i, image_file in tqdm(\n    iterable=enumerate(image_files),\n    total=len(image_files),\n    desc=\"Creating timelapse frames\",\n):\n    current_array = convert_image_to_array(image_file)\n    if stacked_array is None:\n        stacked_array = current_array\n    else:\n        stacked_array = np.maximum(stacked_array, current_array)\n    output_path = os.path.join(output_directory, f\"{i:06d}.jpg\")\n    save_array_as_image(stacked_array, output_path)\n\nprint(\"Created all timelapse frames.\")\n```\n\nBefore running the script, make sure to install `imageio`:\n\n```bash\npip install imageio==2.37.0\n```\n\nOnce you've generated the intermediate blended frames, the next step is to turn them into a time-lapse video. The script below will read each frame you created and combine them into a video using `imageio`. You can control the duration of the video by adjusting the `fps` variable (frames per second).\n\n```python\nimport os\nimport glob\n\nimport imageio.v3 as iio\nfrom tqdm import tqdm\n\ndef get_all_input_images(image_directory: str) -> list[str]:\n    jpg_files = glob.glob(os.path.join(image_directory, \"*.jpg\"))\n    png_files = glob.glob(os.path.join(image_directory, \"*.png\"))\n    image_files = sorted(jpg_files + png_files)\n    return image_files\n\n\nimage_directory = \"./output/timelapse/\"\nimage_files = get_all_input_images(image_directory)\nprint(f\"Found {len(image_files)} image files.\")\n\nif not image_files:\n    raise ValueError(\"No image frames found in the specified directory!\")\n\nprint(\"Creating video from timelapse frames...\")\n\nfps = 60\noutput_directory = \"./output/video/\"\nos.makedirs(os.path.dirname(output_directory), exist_ok=True)\noutput_filename = f\"star_trails_timelapse_{fps}fps.mp4\"\noutput_path = os.path.join(output_directory, output_filename)\nwith iio.imopen(uri=output_path, io_mode=\"w\", plugin=\"pyav\") as writer:\n    writer.init_video_stream(codec=\"libx264\", fps=fps, pixel_format=\"yuv420p\")\n\n    for filename in tqdm(image_files, desc=\"Adding frames to video\"):\n        frame = iio.imread(filename)\n        writer.write_frame(frame)\n\nprint(f\"Video saved successfully to {output_path}\")\n```\n\n---\n\n## Conclusion\n\nIn this article, you learned how to create your own star trail images and time-lapses with Python.\n\nWe covered the theory of how blending works using the `lighten mode`. We turned that theory into a practical Python script using `numpy.maximum`. You also saw how easy it is to adjust that script to create stylized images with comet or fade effects.\n\nBy saving each blended frame, we were able to build a smooth time-lapse that shows the stars moving across the sky. I hope this helps you create your own amazing images.\n\nAll the code we used is available in [this GitHub repository](https://github.com/ImadSaddik/PyStarTrails).\n",
    "view_count": 8,
    "read_count": 4,
    "claps_count": 0
  },
  {
    "id": "cb2c4465-5b88-4f54-927f-f3a9e82ddee0",
    "name": "train-your-own-language-model",
    "tags": [
      "LLM",
      "Transformer",
      "Fine-tuning",
      "Attention",
      "PyTorch",
      "Python",
      "AI",
      "NLP",
      "Machine learning"
    ],
    "creation_date": 1758841200,
    "year": "2025",
    "title": "Train your own language model",
    "type": "course-post",
    "content": "# Train your own language model\n\nLearn every step needed to train a language model from scratch in Python.\n\n**Date:** September 26, 2025\n**Tags:** LLM, Transformer, Fine-tuning, Attention, PyTorch, Python, AI, NLP, Machine learning\n\n---\n\n## Course Overview\n\nThis course is for beginners who want to understand how [large language models](https://en.wikipedia.org/wiki/Large_language_model) (`LLMs`) work and learn how to build one from scratch. We will cover all the steps needed to train a language model, including collecting data, processing it, understanding the [Transformer architecture](<https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)>), pre-training, and [fine-tuning](https://arxiv.org/html/2408.13296v1).\n\nIn this course, I will show you how I trained a language model on [Moroccan Darija](https://en.wikipedia.org/wiki/Moroccan_Arabic) using a dataset from my conversations with friends. Later, I used a larger dataset from [Hugging Face](https://huggingface.co/datasets) and will explain how to work with big datasets without [running out of memory](https://en.wikipedia.org/wiki/Out_of_memory). Finally, I will show you how to fine-tune the base model on a [Supervised Fine-Tuning (SFT)](https://huggingface.co/learn/llm-course/en/chapter11/1) dataset to make the model conversational.\n\nEach step is divided into two parts: `theory` and `practice`. The theory is explained in detail in the [slides](https://github.com/ImadSaddik/Train_Your_Language_Model_Course/blob/main/Slides.pdf), and the practical part is available as a Jupyter notebook on [GitHub](https://github.com/ImadSaddik/Train_Your_Language_Model_Course). The course is easy to follow, and I explain each step along the way.\n\n---\n\n## Watch the Course\n\nI believe knowledge should be free, so I am sharing this course for free on the freeCodeCamp YouTube channel. You can watch the full course below.\n\nCreating courses like this takes months of research, experiments, designing illustrations, recording, and more. If you want to support my mission to share free knowledge, you can also watch the course on [Udemy](https://www.udemy.com/course/train-your-own-language-model/). The Udemy version has extra content, including quizzes, assignments, and a certificate of completion.\n",
    "view_count": 54,
    "read_count": 13,
    "claps_count": 0
  },
  {
    "id": "9d5a8997-8293-4359-9431-4923450d8d92",
    "name": "evolution-of-the-transformer",
    "tags": [
      "LLM",
      "Transformer",
      "Attention",
      "Positional encoding",
      "Normalization",
      "PyTorch",
      "Python",
      "AI",
      "NLP",
      "Machine learning"
    ],
    "creation_date": 1758841200,
    "year": "2025",
    "title": "Evolution of the Transformer architecture from 2017 to 2025",
    "type": "course-post",
    "content": "# Evolution of the Transformer architecture from 2017 to 2025\n\nDiscover how the Transformer architecture has evolved over the years. Implement the different ideas that researchers proposed to improve the original Transformer architecture.\n\n**Date:** September 26, 2025\n**Tags:** LLM, Transformer, Attention, Positional encoding, Normalization, PyTorch, Python, AI, NLP, Machine learning\n\n---\n\n## Course Overview\n\nThis course explores how the [Transformer architecture](<https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)>) has changed from its introduction in the famous paper [Attention is all you need](https://arxiv.org/pdf/1706.03762) in 2017 to the latest updates in 2025. We will look at the main ideas that researchers have used to improve the original Transformer, and you will learn how to build these improvements step by step.\n\nYou will learn both the theory and the practical side. We will cover different ways to encode the position of tokens, new [attention mechanisms](<https://en.wikipedia.org/wiki/Attention_(machine_learning)>), [activation functions](https://en.wikipedia.org/wiki/Activation_function), [normalization methods](<https://en.wikipedia.org/wiki/Normalization_(machine_learning)>), and other ideas that have helped make Transformers better.\n\nYou will use [Python](https://www.python.org/downloads/) and [PyTorch](https://pytorch.org/) to implement everything. I explain each idea in simple terms so anyone can follow along. All course materials, slides, and Jupyter notebooks are available on my [GitHub repository](https://github.com/ImadSaddik/Train_Your_Language_Model_Course).\n\nBy the end of the course, you will build several models using the ideas we discuss. We will compare their performance to see which ones work best. This will help us understand which improvements are most effective.\n\n---\n\n## Watch the Course\n\nI believe knowledge should be free, so I am sharing this course for free on the freeCodeCamp YouTube channel. You can watch the full course below.\n\nCreating courses like this takes months of research, experiments, designing illustrations, recording, and more. If you want to support my mission to share free knowledge, you can also watch the course on [Udemy](https://www.udemy.com/course/train-your-own-language-model/). The Udemy version includes extra content, such as quizzes, assignments, and a certificate of completion.\n",
    "view_count": 4,
    "read_count": 0,
    "claps_count": 0
  },
  {
    "id": "da98bfd4-47f3-44c1-8951-09fccbeb8809",
    "name": "elasticsearch-for-beginners",
    "tags": ["Elasticsearch", "Search", "Python"],
    "creation_date": 1758841200,
    "year": "2025",
    "title": "Elasticsearch for beginners",
    "type": "course-post",
    "content": "# Elasticsearch for beginners\n\nLearn Elasticsearch from scratch with practical examples in Python.\n\n**Date:** September 26, 2025\n**Tags:** Elasticsearch, Search, Python\n\n---\n\n## Course Overview\n\nThis course is for anyone new to [Elasticsearch](https://www.elastic.co/elasticsearch). If you have some experience, you'll still find it helpful for building a strong foundation.\n\nEach topic is explained simply, with practical examples you can try on your own computer. We'll connect to Elasticsearch using [Python](https://www.python.org/downloads/) and run Elasticsearch locally in a [Docker container](https://www.docker.com/resources/what-container/).\n\nYou'll learn about [field data types](https://www.elastic.co/docs/reference/elasticsearch/mapping-reference/field-data-types), how to create an index, add documents, and search for them using different methods like [semantic search](https://en.wikipedia.org/wiki/Semantic_search), [full text search](https://en.wikipedia.org/wiki/Full-text_search), and [hybrid search](https://www.elastic.co/what-is/hybrid-search). You'll also see how to update and delete documents, use aggregations, and much more.\n\nAt the end of the course, we'll put everything together in a [final project](https://www.youtube.com/watch?v=rsC96vtM5K8&t=32s). You'll work with real data and use Elasticsearch to build a great search feature for a web app made just for this course.\n\nAll course materials, slides, and Jupyter notebooks are available on my [GitHub repository](https://github.com/ImadSaddik/ElasticSearch_Python_Course). By the end, you'll understand how Elasticsearch works and how to use it in your own projects.\n\n---\n\n## Watch the Course\n\nI believe knowledge should be free, so I am sharing this course for free on the freeCodeCamp YouTube channel. You can watch the full course below.\n\nCreating courses like this takes months of research, experiments, designing illustrations, recording, and more. If you want to support my mission to share free knowledge, you can also watch the course on [Udemy](https://www.udemy.com/course/train-your-own-language-model/). The Udemy version includes extra content, such as quizzes, assignments, and a certificate of completion.\n",
    "view_count": 1,
    "read_count": 0,
    "claps_count": 0
  },
  {
    "id": "f2e43526-c974-45ac-9471-2dc5b319b9f2",
    "name": "osrm-for-beginners",
    "tags": ["OSRM", "Routing", "Python", "Docker", "TSP"],
    "creation_date": 1758841200,
    "year": "2025",
    "title": "OSRM for beginners",
    "type": "course-post",
    "content": "# OSRM for beginners\n\nLearn the different ways to use OSRM (Open Source Routing Machine) with practical examples in Python.\n\n**Date:** September 26, 2025\n**Tags:** OSRM, Routing, Python, Docker, TSP\n\n---\n\n## Course Overview\n\nThis course will teach you how to use [OSRM (Open Source Routing Machine)](https://github.com/Project-OSRM/osrm-backend) for different routing tasks. OSRM is a free and powerful tool that helps you find the shortest route between places, solve the [traveling salesman problem (TSP)](https://en.wikipedia.org/wiki/Travelling_salesman_problem), and much more.\n\nYou'll learn how to set up OSRM on your computer using [Docker](https://www.docker.com/), download a map area to focus your searches, and use OSRM with [Python](https://www.python.org/downloads/). The course is made for beginners, so you don't need any previous experience with OSRM or routing tools.\n\nHere's what you'll learn:\n\n- How to set up OSRM with Docker.\n- How to download and prepare map data from any region in the world.\n- How to find the shortest path between two points.\n- How to generate a distance matrix (table) between multiple locations.\n- How to solve the Traveling Salesman Problem (TSP).\n\nAll course materials, slides, and Jupyter notebooks are available on my [GitHub repository](https://github.com/ImadSaddik/OSRM_Course_Python). By the end of the course, you'll know how OSRM works and how to use it in your own projects.\n\n---\n\n## Watch the Course\n\nI believe knowledge should be free, so I am sharing this course for free on the freeCodeCamp YouTube channel. You can watch the full course below.\n\nCreating courses like this takes months of research, experiments, designing illustrations, recording, and more. If you want to support my mission to share free knowledge, you can also watch the course on [Udemy](https://www.udemy.com/course/train-your-own-language-model/). The Udemy version includes extra content, such as quizzes, assignments, and a certificate of completion.\n",
    "view_count": 1,
    "read_count": 0,
    "claps_count": 0
  },
  {
    "id": "8f01cfd6-054f-490b-b030-4d89de20b65c",
    "name": "fix-kernel-panic-linux",
    "tags": ["Linux", "Ubuntu", "Kernel panic", "GRUB", "Troubleshooting"],
    "creation_date": 1766185200,
    "year": "2025",
    "title": "How to fix the kernel panic problem after installing a new version of the kernel",
    "type": "blog-post",
    "content": "# How to fix the kernel panic problem after installing a new version of the kernel\n\nA step-by-step guide to recovering your Linux system from a broken kernel update using GRUB.\n\n**Date:** December 20, 2025\n\n**Tags:** Linux, Ubuntu, Kernel panic, GRUB, Troubleshooting\n\n---\n\n## Introduction\n\nWhile using my Ubuntu laptop one night, I started an update to the newest Linux kernel. I was exhausted, so once the process finished, I went to sleep immediately without looking to see if it was successful.\n\nThat turned out to be a big mistake. I discovered this the next morning when I turned on my laptop and was greeted by a bright pink screen with the words: `KERNEL PANIC!` The installation had failed, and the new kernel was not working correctly.\n\nAt first, I didn't know what to do, so I just kept restarting my laptop, hoping the problem would disappear. When that failed, I tried booting into Windows from the GRUB menu, and it worked perfectly.\n\nThis confirmed that the issue was caused by the kernel update I had run the day before. With the problem identified, I started researching and thankfully found a way to fix my Linux installation without losing any data.\n\nIn this article, I will share the exact steps I took to fix this problem. My goal is to give you a clear solution to fix this error if you ever run into it.\n\n---\n\n## The GRUB menu\n\nAfter turning on the laptop, the first screen you'll see is the [GNU GRUB](https://en.wikipedia.org/wiki/GNU_GRUB) menu. GRUB is a **[bootloader](https://en.wikipedia.org/wiki/Bootloader)**, a program that lets you choose which operating system to start.\n\nIf you have more than one OS, it allows you to choose which one to boot. We will use this menu to select a special option that lets us start Ubuntu with an older kernel, bypassing the one that is causing the panic.\n\n_The GNU GRUB menu._\n\nThe first option in the list is `Ubuntu`. This option is automatically set to boot the newest kernel you have installed. Since the newest kernel is the broken one, selecting this will take you right back to the pink kernel panic screen. We need to choose a different option instead.\n\n_The scary kernel panic screen!_\n\n---\n\n## Fixing the problem\n\n### Boot into a working Linux kernel\n\nFrom the main GRUB menu, use your arrow keys to select `Advanced options for Ubuntu` and press Enter. This will take you to a new screen listing all the Linux kernels installed on your system.\n\nYou will see at least two versions. For example, my list showed a new, broken kernel (`6.14.0-24-generic`) and an older, working one (`6.11.0-29-generic`).\n\n_The different versions of the Linux kernel._\n\nThe menu doesn't label which kernel is broken. You may need to do a quick test to find a working version. Simply select one from the list and press Enter. If you see the kernel panic screen again, just reboot your computer and try a different one until your system starts successfully.\n\n### Remove the broken kernel\n\nNow that your system is running on a working kernel, it's time to remove the broken one to clean your system.\n\nFirst, we need to know exactly which kernel you are currently using. This is the safe kernel that you must **not** delete. Open a terminal and run this command:\n\n```bash\nuname -r\n\n```\n\nThe output will be your current kernel's version.\n\n```text\n6.11.0-29-generic\n\n```\n\nNext, list all the kernel packages on your system to find the broken one.\n\n```bash\ndpkg --list | grep linux-image\n\n```\n\nYou will see a list of kernels. Look for letters like `ii` or `iF` at the beginning of each line.\n\n- `ii` means the kernel is correctly installed. Your working kernel from the previous step should have `ii` next to it.\n- `iF` means the kernel installation failed, and it's in a broken state. **This is the one we need to remove.**\n- `rc` means the kernel was removed, but its configuration files are still there.\n\n```text\nrc linux-image-6.11.0-17-generic 6.11.0-17.17~24.04.2 amd64 Signed kernel image generic\nrc linux-image-6.11.0-19-generic 6.11.0-19.19~24.04.1 amd64 Signed kernel image generic\nrc linux-image-6.11.0-21-generic 6.11.0-21.21~24.04.1+1 amd64 Signed kernel image generic\nrc linux-image-6.11.0-24-generic 6.11.0-24.24~24.04.1 amd64 Signed kernel image generic\nrc linux-image-6.11.0-25-generic 6.11.0-25.25~24.04.1 amd64 Signed kernel image generic\nrc linux-image-6.11.0-26-generic 6.11.0-26.26~24.04.1 amd64 Signed kernel image generic\nrc linux-image-6.11.0-28-generic 6.11.0-28.28~24.04.1 amd64 Signed kernel image generic\nii linux-image-6.11.0-29-generic 6.11.0-29.29~24.04.1 amd64 Signed kernel image generic\niF linux-image-6.14.0-24-generic 6.14.0-24.24~24.04.3 amd64 Signed kernel image generic\nrc linux-image-6.8.0-41-generic 6.8.0-41.41 amd64 Signed kernel image generic\nrc linux-image-6.8.0-44-generic 6.8.0-44.44 amd64 Signed kernel image generic\nrc linux-image-6.8.0-45-generic 6.8.0-45.45 amd64 Signed kernel image generic\nrc linux-image-6.8.0-47-generic 6.8.0-47.47 amd64 Signed kernel image generic\nrc linux-image-6.8.0-48-generic 6.8.0-48.48 amd64 Signed kernel image generic\nrc linux-image-6.8.0-49-generic 6.8.0-49.49 amd64 Signed kernel image generic\nrc linux-image-6.8.0-50-generic 6.8.0-50.51 amd64 Signed kernel image generic\nrc linux-image-6.8.0-51-generic 6.8.0-51.52 amd64 Signed kernel image generic\nrc linux-image-6.8.0-52-generic 6.8.0-52.53 amd64 Signed kernel image generic\nii linux-image-generic-hwe-24.04 6.14.0-24.24~24.04.3 amd64 Generic Linux kernel image\n\n```\n\nNow we will completely remove the broken kernel and its matching header files. We use `apt purge` because it deletes the configuration files as well.\n\n> **Warning:** Double-check that this is **NOT** the version you are currently using (from the `uname -r` command).\n\n```bash\nsudo apt purge linux-image-6.14.0-24-generic linux-headers-6.14.0-24-generic\n\n```\n\nAfter removing the kernel, run `autoremove` to get rid of any other packages that are no longer needed.\n\n```bash\nsudo apt autoremove --purge\n\n```\n\nFinally, update your GRUB menu and restart the computer to make sure everything is perfect.\n\n```bash\nsudo update-grub\n\n```\n\nNow, reboot your laptop to make sure that everything is set correctly. It should boot directly into your 6.11 kernel when clicking on `Ubuntu` in the GRUB menu.\n\n_Success! The system is working again._\n\n---\n\n## Conclusion\n\nAnd that's it! By using the GRUB menu to boot into an older, working kernel, you can get your system back online safely. From there, it only takes a few terminal commands to remove the broken kernel and clean your system.\n\nI hope this guide helped you solve the issue.\n",
    "view_count": 114,
    "read_count": 11,
    "claps_count": 0
  },
  {
    "id": "7d527834-1fe6-416c-9aad-cf37569c2882",
    "name": "inkscape-clean-up-document",
    "tags": ["Inkscape", "SVG", "Optimization"],
    "creation_date": 1766271600,
    "year": "2025",
    "title": "How to remove hidden data from Inkscape files to reduce the file size",
    "type": "blog-post",
    "content": "# How to remove hidden data from Inkscape files to reduce the file size\n\nA simple guide to removing invisible data from embedded images to keep your Inkscape projects lightweight.\n\n**Date:** December 21, 2025\n**Tags:** Inkscape, SVG, Optimization\n\n---\n\n## Introduction\n\nI recently ran into an issue where an Inkscape file I kept reusing grew from just 12KB to over 12MB. The problem was that embedded images remained hidden in the file even after I had deleted them from the canvas.\n\nThis guide will show you a simple fix to remove this invisible data to keep your files small.\n\n## The problem\n\nThis issue often happens when working with **embedded** [raster images](https://en.wikipedia.org/wiki/Raster_graphics) (like PNGs or JPEGs). When you delete an image from the canvas, Inkscape doesn't always remove the image's data from the SVG file itself.\n\nYou won't see the deleted image on your canvas or even in the `Layers and Objects` panel, but the data remains hidden in the file's code, keeping the size unnecessarily large.\n\n_The **Layers and Objects** panel is not showing the images._\n\nIf you open the SVG file in a text editor, you will find the image data still there. It is encoded in a format called [base64](https://en.wikipedia.org/wiki/Base64) and looks something like this:\n\n`data:image/png;base64,iVBORw0KGgoAAAANS...`\n\n### Replicate the issue\n\nFirst, drag a large raster image into your Inkscape document and select `Embed` and `Default import resolution`.\n\n_The image import dialog._\n\nAdd another rectangle and try clipping the image using the shape builder tool. Select both the rectangle and the image, then press `X` or click the tool directly. Choose the part of the image you want to keep, and click the `Accept` button to confirm.\n\n_Use the shape builder tool to clip the image._\n\nNow, save the file and close Inkscape. Check the file size, you should notice it has increased because we added a new image. In my case, it went from 12 MB to 17.5 MB.\n\nNext, open the document again in Inkscape, delete the image, and save the file. Close or minimize Inkscape and check the file size once more. It didn't change, right? That's the problem, the file is still 17.5 MB because Inkscape keeps storing images that are no longer in use.\n\n## The solution\n\nLuckily, there is an easy fix to this issue. Click on `File > Clean Up Document` and save the file. This option removes any unused elements from your document.\n\n_The Clean up document option._\n\nDid the file size change? In my case, it shrank to only 12KB. That is a **1458x** reduction.\n\n## Conclusion\n\nAs you've seen, the `Clean Up Document` feature removes unused data, such as deleted raster images, that can make your files unnecessarily large.\n\nI hope this guide helps you keep your Inkscape projects clean.\n",
    "view_count": 11,
    "read_count": 3,
    "claps_count": 15
  },
  {
    "id": "d0e7dd51-65ba-46f0-863d-e933f2e2eefa",
    "name": "local-ai-stack-on-linux",
    "tags": [
      "Linux",
      "AI",
      "LLM",
      "llama.cpp",
      "LibreChat",
      "Local AI",
      "llama-swap"
    ],
    "creation_date": 1766790000,
    "year": "2025",
    "title": "How to build your own local AI stack on Linux with llama.cpp, llama-swap, LibreChat and more",
    "type": "blog-post",
    "content": "# How to build your own local AI stack on Linux with llama.cpp, llama-swap, LibreChat and more\n\nA complete guide to running LLMs, embedding models, and multimodal models locally with full control and automation.\n\n**Date:** October 5, 2025\n**Tags:** Linux, AI, LLM, llama.cpp, LibreChat, Local AI, llama-swap\n\n---\n\n## Introduction\n\nI wrote this article to document every step I took to replace [ollama](https://github.com/ollama/ollama) with [llama.cpp](https://github.com/ggml-org/llama.cpp), and [llama-swap](https://github.com/mostlygeek/llama-swap). Along the way, I discovered other interesting projects like [whisper.cpp](https://github.com/ggml-org/whisper.cpp), [faster-whisper](https://github.com/SYSTRAN/faster-whisper), and [LibreChat](https://github.com/danny-avila/LibreChat).\n\nThis article is long because I show everything in detail. You will learn a lot from this article. You will build `llama.cpp` to run [LLM](https://en.wikipedia.org/wiki/Large_language_model)s, and [multimodal models](https://www.ibm.com/think/topics/multimodal-ai). You will use `whisper.cpp` and `faster-whisper` to run [the whisper model](https://github.com/openai/whisper) from OpenAI.\n\n`llama-swap` is a handy tool that you will use as a middleman between the backend and frontend. The backend is a server that will host the model, and the frontend in our case is `LibreChat`.\n\nIn the end, I will show you how to automate everything by using [services](https://wiki.archlinux.org/title/Systemd), [cron jobs](https://en.wikipedia.org/wiki/Cron), creating desktop shortcuts, and using watchers to detect file changes to run commands automatically.\n\n_The architecture of the local AI stack that you will build._\n\n> [!IMPORTANT]\n>\n> Throughout this guide, I use my own username (`imad-saddik`) and specific directory paths (e.g., `/home/imad-saddik/...`) in the code snippets.\n>\n> You **must** update these paths to match your own username and system file structure. Additionally, check that flags like `-DCMAKE_CUDA_ARCHITECTURES` match your specific GPU model.\n\n## Hardware and operating system\n\nThe experiments that I conducted in this article were performed on an **Asus ROG Strix G16 gaming laptop** running **Ubuntu 24.04.2 LTS** with the following configuration:\n\n- **System RAM**: 32GB\n- **CPU**: 13th Gen IntelÂ® Coreâ„¢ i7â€“13650HX Ã— 20\n- **GPU**: NVIDIA GeForce RTXâ„¢ 4070 Laptop GPU (8GB of VRAM)\n\n## Installing llama.cpp\n\nThe main reason that made me want to try `llama.cpp` is running the new [Qwen3-30B-A3B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507) model released by the [Qwen team](https://qwen.ai/home).\n\n_Comparing Qwen3â€“30B-A3B-Instruct-2507 to other models on selected benchmarks. Graph shared by the Qwen team._\n\nOllama is great, but it decides for you how it should run a model. That worked well for models that didnâ€™t exceed the **12B parameters** range. However, when trying to run big models, ollama complains that I donâ€™t have enough resources to use those models.\n\nFor months I was stuck in that situation, I was limited to running only **8â€“12B parameter** models with ollama. But this weekend, I decided to try `llama.cpp` after hearing people talk about it on Reddit, YouTube videos, and articles.\n\nYou have plenty of options to install `llama.cpp`, [refer to the README file on GitHub](https://github.com/ggml-org/llama.cpp/blob/master/README.md). I decided to build `llama.cpp` from source for my hardware. You can [follow the build guide](https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md) that the team behind `llama.cpp` has created.\n\nI have an NVIDIA GPU, I will follow the [CUDA section](https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#cuda) in the build guide. If you have something else, make sure to follow the section that discusses how to build `llama.cpp` for your hardware.\n\n### Installing build tools\n\nOn Ubuntu, run these commands to install the build tools.\n\n```bash\nsudo apt update\nsudo apt install build-essential cmake git\n```\n\n### NVIDIA CUDA Toolkit\n\nYou need to install the **NVIDIA CUDA Toolkit** if you donâ€™t have it. This is very important if you want to use your GPU for inference. You can [install the toolkit from NVIDIAâ€™s website](https://developer.nvidia.com/cuda-downloads).\n\n_Selecting the target platform before downloading the CUDA toolkit._\n\nWhen visiting the website, you will be prompted to select your operating system, architecture, distribution, and other stuff. The options that are selected in the image are valid for my system. Please make sure to select what is valid for you.\n\nFor the installation type, you can choose **deb (network)** because it automatically handles downloading and installing all the necessary dependencies.\n\n_The commands to install the CUDA toolkit will be shown after completing the first step._\n\nAfter completing the first step, you will see a list of commands that you need to run in order to install the toolkit. To verify that the installation went well, run this command.\n\n```bash\nnvcc --version\n```\n\nYou should see something like this.\n\n```text\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Fri_Jan__6_16:45:21_PST_2023\nCuda compilation tools, release 12.0, V12.0.140\nBuild cuda_12.0.r12.0/compiler.32267302_0\n```\n\n### Build llama.cpp with CUDA support\n\nStart by cloning the `llama.cpp` project from GitHub with the following command.\n\n```bash\ngit clone [https://github.com/ggml-org/llama.cpp.git](https://github.com/ggml-org/llama.cpp.git)\n```\n\nNow, go inside the `llama.cpp` folder.\n\n```bash\ncd llama.cpp\n```\n\nThe first command that we need to run is the following.\n\n```bash\ncmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=89\n```\n\nWe run the `cmake` command with the `GGML_CUDA=ON` flag to enable support for NVIDIA CUDA so that we can use the GPU for inference.\n\nThe `CMAKE_CUDA_ARCHITECTURES` flag tells the compiler exactly which NVIDIA GPU architecture to build the code for. I have an RTX 4070 and the compute capability of this GPU is 8.9. [Visit this page](https://developer.nvidia.com/cuda-gpus) to check the compute capability of your GPU.\n\nAfter running that command, you should see something like this.\n\n```text\n$ cmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=89\n\n-- The C compiler identification is GNU 13.3.0\n-- The CXX compiler identification is GNU 13.3.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/cc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\nCMAKE_BUILD_TYPE=Release\n-- Found Git: /usr/bin/git (found version \"2.43.0\")\n-- The ASM compiler identification is GNU\n-- Found assembler: /usr/bin/cc\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n-- Found Threads: TRUE\n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\n-- GGML_SYSTEM_ARCH: x86\n-- Including CPU backend\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n-- Found OpenMP: TRUE (found version \"4.5\")\n-- x86 detected\n-- Adding CPU backend variant ggml-cpu: -march=native\n-- Found CUDAToolkit: /usr/include (found version \"12.0.140\")\n-- CUDA Toolkit found\n-- Using CUDA architectures: 89\n-- The CUDA compiler identification is NVIDIA 12.0.140\n-- Detecting CUDA compiler ABI info\n-- Detecting CUDA compiler ABI info - done\n-- Check for working CUDA compiler: /usr/bin/nvcc - skipped\n-- Detecting CUDA compile features\n-- Detecting CUDA compile features - done\n-- CUDA host compiler is GNU 12.3.0\n-- Including CUDA backend\n-- ggml version: 0.9.0-dev\n-- ggml commit:  3f81b4e9\n-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"8.5.0\")\n-- Configuring done (2.6s)\n-- Generating done (0.1s)\n-- Build files have been written to: /home/imad-saddik/Projects/llama.cpp/build\n```\n\nThe build files were created. Now, we can build `llama.cpp`. Run this command.\n\n```bash\ncmake --build build --config Release -j $(nproc)\n```\n\nThe command takes the configuration files from the `build` directory and starts the compilation and linking process to create the final program. To speed up the compilation I added `$(nproc)` to use all available CPU cores.\n\nYou should see something like this in the end.\n\n```text\n$ cmake --build build --config Release -j $(nproc)\n\n...\n\n[ 98%] Built target llama-tts\n[ 99%] Linking CXX executable ../bin/test-backend-ops\n[ 99%] Built target test-backend-ops\n[100%] Linking CXX executable ../../bin/llama-server\n[100%] Built target llama-server\n```\n\nThe compiled programs are now located in the `build/bin/` directory. Letâ€™s copy them to `/usr/local/bin`.\n\n```bash\nsudo cp build/bin/llama-cli /usr/local/bin/\nsudo cp build/bin/llama-server /usr/local/bin/\nsudo cp build/bin/llama-embedding /usr/local/bin/\n```\n\nNow, you can run `llama-cli`, `llama-embedding` and `llama-server` directly from any location.\n\n> [!NOTE]\n> If you recompile the project, make sure to copy the programs again.\n\n## Running an LLM\n\nCompanies and research teams upload their models to [Hugging Face](https://huggingface.co/) in the `safetensors` format.\n\nSearch for the [Qwen3-30B-A3B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507) model in the hub. Click on the [Files and versions](https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507/tree/main) tab, notice how many `safetensors` files are there. These files contain the weights in their original high-precision format (16 or 32-bits).\n\n_The Qwen3-30B-A3B-Instruct-2507 model uploaded to Hugging Face by the Qwen team._\n\n`llama.cpp` does not work with the `safetensors` format, it works with the [GGUF format](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md). This format is optimized for quick loading and saving of models, and running models efficiently on consumer hardware.\n\nThe good news is that we can convert models to the GGUF format. There is a [Hugging Face space](https://huggingface.co/spaces/ggml-org/gguf-my-repo) that you can use for that purpose, or this [python script](https://github.com/ggml-org/llama.cpp/blob/master/convert_hf_to_gguf.py).\n\nIf you donâ€™t want to convert the model by yourself, you can search for GGUF versions of the model that you want to download. There are a lot of people that do this for the community. You have [Unsloth](https://huggingface.co/unsloth/models), [bartowski](https://huggingface.co/bartowski), [TheBloke](https://huggingface.co/TheBloke), and others. Sometimes, the teams behind the models release GGUF versions too.\n\n_A GGUF version of the Qwen3-30B-A3B-Instruct-2507 model from Unsloth._\n\nHugging Face displays useful information for GGUF models. On the right side, you will find how much memory is needed to run the model at different [quantization levels](https://huggingface.co/docs/hub/gguf#quantization-types).\n\n_The hardware compatibility panel._\n\nHugging Face is displaying the available quantization levels for the model that you selected. You will see a green checkmark next to the quantization levels that you can run without any issue on your system.\n\nIf you donâ€™t see those icons that means that you did not specify the hardware that you have. Click on your profile picture, then settings.\n\nIn the settings page, click on `Local Apps and Hardware`. After that, select one of the three options under the `Add new Hardware` section, select the model that you have and click on the `Add item` button.\n\n_Adding Hardware to your Hugging Face profile._\n\nBut, Imad! I see a lot of quantization options, which one should I choose? That is a great question, before I answer it I would like you to [read this page](https://huggingface.co/docs/hub/gguf#quantization-types) from Hugging Face. There is a table that explains all those quantization types in detail.\n\n_The quantization types explained in the Hugging Face docs._\n\nAvoid downloading models in `FP32` or `FP16` precision, as these unquantized formats require a lot of memory, especially for very large models.\n\nInstead, download **quantized** versions of the model in the **GGUF format**, because they use less memory. A great starting point is the `Q8_K` quantization level.\n\nIf you encounter an **[Out Of Memory](https://en.wikipedia.org/wiki/Out_of_memory)** **(OOM)** error or the model runs too slowly, try a more aggressive quantization like `Q6_K`. Repeat this process with progressively smaller versions until you find one that runs at a **decent** speed on your machine.\n\nTo stay organized, create a dedicated folder for your downloaded models. I will store mine in `~/.cache/llama.cpp/`, but you can use any directory you prefer.\n\nTo run a model, use the `llama-cli` command. The following example shows how to run the `Qwen3-30B-A3B-Thinking-2507-IQ4_XS` model with custom settings that I modified.\n\n```bash\nllama-cli \\\n  --model ~/.cache/llama.cpp/Qwen3-30B-A3B-Thinking-2507-IQ4_XS.gguf \\\n  --n-gpu-layers 20 \\\n  --ctx-size 4096 \\\n  --color \\\n  --interactive-first\n```\n\nThis command uses several arguments to control the modelâ€™s behavior. Hereâ€™s an explanation of the arguments that I used:\n\n- `--ctx-size`: Sets the model's context window size in tokens. Larger values can handle longer conversations but require more memory.\n- `--n-gpu-layers`: Offloads a specified number of the model's layers to the GPU for acceleration. Adjust this based on your GPU's VRAM.\n- `--color`: Adds color to the output to distinguish your input from the model's generation.\n- `--interactive-first`: Starts in interactive mode, waiting for user input immediately instead of processing an initial prompt.\n\nFor a complete list of all available arguments and their descriptions, run the help command.\n\n```bash\nllama-cli --help\n```\n\nAfter running the `llama-cli` command, the program will load the model and display startup information before presenting an interactive prompt. You can then begin chatting with the model directly in your terminal.\n\n```text\n$ llama-cli \\\n  --model ~/.cache/llama.cpp/Qwen3-30B-A3B-Thinking-2507-IQ4_XS.gguf \\\n  --n-gpu-layers 20 \\\n  --ctx-size 4096 \\\n  --color \\\n  --interactive-first\n\n...\n\n== Running in interactive mode. ==\n\n- Press Ctrl+C to interject at any time.\n- Press Return to return control to the AI.\n- To return control without starting a new line, end your input with '/'.\n- If you want to submit another line, end your input with '\\'.\n- Not using system message. To change it, set a different value via -sys PROMPT\n\n> Hi\n> Hello! ðŸ˜Š How can I assist you today?\n\n>\n```\n\nBefore the interactive prompt appears, `llama-cli` outputs detailed logs about the model loading process. If you scroll up through this output, you can verify that the **GPU was detected** and that the 20 layers of the model were successfully offloaded.\n\nFirst, look for a message confirming `llama.cpp` found your CUDA device.\n\n```text\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4070 Laptop GPU, compute capability 8.9, VMM: yes\n```\n\nNext, you can see exactly how many of the modelâ€™s layers were offloaded to the GPU. In this case, 20 out of 49 layers were moved.\n\n```text\nload_tensors: offloading 20 repeating layers to GPU\nload_tensors: offloaded 20/49 layers to GPU\n```\n\nFinally, the logs provide a breakdown of memory allocation. This is useful for understanding how much **VRAM** and **RAM** the model is using.\n\n```text\nprint_info: file size = 15.25 GiB (4.29 BPW)\n\nload_tensors:      CUDA0 model buffer size = 6334.71 MiB\nload_tensors: CPU_Mapped model buffer size = 9278.95 MiB\n```\n\nBased on this output:\n\n- The 20 offloaded layers are occupying **~6.3 GB of GPU VRAM**.\n- The rest of the model is running on the CPU and consuming **~9.3 GB of system RAM**.\n\nWhen you exit the interactive session by pressing `Ctrl+C`, `llama-cli` automatically displays a detailed performance report. The output will look like this.\n\n```text\nllama_perf_sampler_print:    sampling time =      25.55 ms /   390 runs   (    0.07 ms per token, 15264.79 tokens per second)\nllama_perf_context_print:        load time =   12043.37 ms\nllama_perf_context_print: prompt eval time =     444.75 ms /    24 tokens (   18.53 ms per token,    53.96 tokens per second)\nllama_perf_context_print:        eval time =   23582.66 ms /   602 runs   (   39.17 ms per token,    25.53 tokens per second)\nllama_perf_context_print:       total time =  737557.31 ms /   626 tokens\n```\n\nHere is how to interpret the most important metrics from this report:\n\n- `load time`: This is the time it took to load the large model file from your disk into RAM and VRAM. In my case, it took 12 seconds.\n- `prompt eval time`: This measures the speed of processing your initial prompt. Here, the speed was `53.96 tokens per second`.\n- `eval time`: This is the generation speed, which shows how quickly the model produces new tokens after processing your prompt. In my case, the model generated text at a speed of `25.53 tokens per second`.\n\n## Serving the LLM as an API\n\nServing a model means running it as a local server, exposing it as an **API** that other applications can connect to and use.\n\nFor example, you could write a Python program that uses the model for a specific task or connect a web interface like LibreChat for a more user-friendly chat experience.\n\nTo serve the model as an API, we use the `llama-server` program. The command is very similar to `llama-cli`, but with additional arguments for networking.\n\nHere is how you can serve the `Qwen3-30B-A3B-Thinking-2507-IQ4_XS` model.\n\n```bash\nllama-server \\\n  --model ~/.cache/llama.cpp/Qwen3-30B-A3B-Thinking-2507-IQ4_XS.gguf \\\n  --n-gpu-layers 20 \\\n  --host 0.0.0.0 \\\n  --port 8080 \\\n  --ctx-size 4096\n```\n\nIn this command we use two networking arguments:\n\n- `--host`: Specifies the IP address the server will listen on.\n- `--port`: Specifies the network port the server will use.\n\nWhy use `--host 0.0.0.0`?\n\nSetting the host to `0.0.0.0` makes the server listen on all available network interfaces. This is very important for allowing other services, especially applications running inside **Docker containers** (like LibreChat), to see and connect to your `llama-server` instance.\n\nIf you were to use the default `127.0.0.1` (localhost), the server would only be accessible from your main computer, and the Docker container would not be able to reach it.\n\nAfter running the command, the log output will confirm that the server is running and ready to accept requests.\n\n```text\nmain: server is listening on [http://0.0.0.0:8080](http://0.0.0.0:8080) - starting the main loop\nsrv update_slots: all slots are idle\n```\n\n### Connect to the server with Python\n\nSince `llama-server` provides an [OpenAI-compatible API](https://github.com/openai/openai-openapi), you can interact with it using the same tools you would use for OpenAI's models. The endpoint for chat is `/v1/chat/completions`.\n\nLetâ€™s write two Python scripts to connect to our server: one for a simple request-response, and a second example for streaming the response in real-time.\n\n#### Example 1: A simple request\n\nIn this example, we send a prompt to the server and wait for the full response to be generated before printing it.\n\n- We use [requests](https://github.com/psf/requests) to send the HTTP request and `json` to format our data.\n- We create a `messages` list containing `system` and `user` roles. This is sent inside a `data` dictionary, along with other parameters like `temperature`.\n- We send a POST request to the server.\n- We parse the returned JSON to extract and print the modelâ€™s message.\n\n```python\nimport json\n\nimport requests\n\ntry:\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"Hello! What is the capital of Morocco?\",\n        },\n    ]\n\n    data = {\"messages\": messages, \"temperature\": 0.7}\n    response = requests.post(\n        url=\"http://localhost:8080/v1/chat/completions\",\n        headers={\n            \"Content-Type\": \"application/json\",\n        },\n        data=json.dumps(data),\n    )\n    response.raise_for_status()\n\n    response_json = response.json()\n    assistant_message = response_json[\"choices\"][0][\"message\"][\"content\"]\n    print(assistant_message.strip())\n\nexcept requests.exceptions.RequestException as e:\n    print(f\"An error occurred: {e}\")\n```\n\n#### Example 2: Streaming the response\n\nFor a real-time experience, you can stream the response token by token. The server uses a standard format called [Server-Sent Events (SSE)](https://html5doctor.com/server-sent-events/). Hereâ€™s how it works:\n\n- We add `\"stream\": True` to our JSON payload and `stream=True` to the `requests.post()` call.\n- We use `response.iter_lines()` to process the response as it arrives.\n- Each piece of data from the stream is a line of bytes prefixed with `data:`. The code decodes the line, strips the prefix, and parses the remaining string.\n- The stream ends when the server sends a final message of `data: [DONE]`. We break out of the loop when we receive that message.\n- The generated token is stored inside the **delta** object. We extract this token and print it immediately using `end=\"\"` and `flush=True` to display it on the same line.\n\n```python\nimport json\n\nimport requests\n\ntry:\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"Hello! What is the capital of Morocco.\",\n        },\n    ]\n\n    data = {\n        \"messages\": messages,\n        \"temperature\": 0.7,\n        \"stream\": True,\n    }\n    response = requests.post(\n        url=\"http://localhost:8080/v1/chat/completions\",\n        headers={\"Content-Type\": \"application/json\"},\n        data=json.dumps(data),\n        stream=True,\n    )\n    response.raise_for_status()\n\n    for line in response.iter_lines():\n        if not line:\n            continue\n\n        decoded_line = line.decode(\"utf-8\")\n\n        # Each line is prefixed with \"data: \", so we strip that\n        if decoded_line.startswith(\"data: \"):\n            json_string = decoded_line[len(\"data: \") :]\n            if json_string.strip() == \"[DONE]\":\n                break\n\n            try:\n                chunk = json.loads(json_string)\n\n                delta_object = chunk[\"choices\"][0][\"delta\"]\n                if \"content\" in delta_object:\n                    token = delta_object[\"content\"]\n                    if token:\n                        print(token, end=\"\", flush=True)\n            except json.JSONDecodeError:\n                pass\n\n    print()\n\nexcept requests.exceptions.RequestException as e:\n    print(f\"\\nAn error occurred: {e}\")\n```\n\n### Connect to the server with LibreChat\n\nLLMs format their responses in **[Markdown](https://en.wikipedia.org/wiki/Markdown)**. This makes it hard to read the output in the terminal. This is why using web interfaces like **LibreChat** is necessary because they format the output in a way that makes it easier to read.\n\nI love LibreChat because it is highly customizable and can connect to any AI provider, not just our local `llama.cpp` server. It can display the **thinking** process for reasoning models. It supports RAG, storing memories about you, and much more.\n\nNow, letâ€™s configure LibreChat to work with our running `llama-server`. First, clone the LibreChat project and navigate into the new directory.\n\n```bash\ngit clone [https://github.com/danny-avila/LibreChat.git](https://github.com/danny-avila/LibreChat.git)\ncd LibreChat\n```\n\nBefore continuing, make sure that you have [Docker](https://www.docker.com/) installed. Next, create your custom configuration file by copying the example file.\n\n```bash\ncp librechat.example.yaml librechat.yaml\n```\n\nNow, open the new `librechat.yaml` file. Under the `endpoints.custom` section, add the following entry. This tells LibreChat how to connect to our local `llama.cpp` server.\n\n```yaml\nendpoints:\n  custom:\n    - name: \"llama.cpp\"\n      apiKey: \"llama-cpp-is-awesome\" # Put anything here. Because we are running the models locally, there is no API key that we need to provide.\n      baseURL: \"[http://host.docker.internal:8080/v1](http://host.docker.internal:8080/v1)\"\n      models:\n        default: [\n            \"canis-majoris\", # Put any model name here for now. We will change this later.\n          ]\n        fetch: false\n      titleConvo: true\n      titleModel: \"current_model\"\n      modelDisplayLabel: \"Llama.cpp\"\n```\n\nThe most important setting here is `baseURL`. The address `http://host.docker.internal:8080/v1` allows the LibreChat container to communicate with the `llama-server` running on your main computer. The `apiKey` can be set to anything you like.\n\nNext, create a [Docker compose override file](https://www.librechat.ai/docs/configuration/docker_override). This will tell Docker to use your custom configuration.\n\n```bash\ncp docker-compose.override.yml.example docker-compose.override.yml\n```\n\nOpen the new `docker-compose.override.yml` and uncomment the `services` section. This configuration mounts your `librechat.yaml` file into the container and sets the image for the `api` service to `ghcr.io/danny-avila/librechat:latest`, which pulls the latest stable LibreChat image.\n\n```yaml\n# Please consult our docs for more info: [https://www.librechat.ai/docs/configuration/docker_override](https://www.librechat.ai/docs/configuration/docker_override)\n\n# TO USE THIS FILE, FIRST UNCOMMENT THE LINE ('services:')\n\n# THEN UNCOMMENT ONLY THE SECTION OR SECTIONS CONTAINING THE CHANGES YOU WANT TO APPLY\n# SAVE THIS FILE AS 'docker-compose.override.yaml'\n# AND USE THE 'docker compose build' & 'docker compose up -d' COMMANDS AS YOU WOULD NORMALLY DO\n\n# WARNING: YOU CAN ONLY SPECIFY EVERY SERVICE NAME ONCE (api, mongodb, meilisearch, ...)\n# IF YOU WANT TO OVERRIDE MULTIPLE SETTINGS IN ONE SERVICE YOU WILL HAVE TO EDIT ACCORDINGLY\n\n# EXAMPLE: if you want to use the config file and the latest numbered release docker image the result will be:\n\nservices:\n  api:\n    volumes:\n      - type: bind\n        source: ./librechat.yaml\n        target: /app/librechat.yaml\n    image: ghcr.io/danny-avila/librechat:latest\n```\n\nYou can now build and run the LibreChat application using Docker compose. Run the following command.\n\n```bash\ndocker compose up -d\n```\n\n_Starting the LibreChat application._\n\nOpen your web browser and navigate to [http://localhost:3080](https://www.google.com/search?q=http://localhost:3080). After that, create an account, and login. To select a model that you are serving with `llama-server` follow these steps:\n\n- Click the model selector button in the top-left corner.\n- In the dropdown menu, hover over `llama.cpp`.\n- Select the model name that appears.\n\n_Selecting a model from Llama.cpp._\n\nYouâ€™ll notice the model is named `canis-majoris`. This is just a display name that I chose, it has nothing to do with the model that you are serving with `llama-server`.\n\nYou can now start chatting with your locally hosted model! Donâ€™t forget to run the server first, here is the command.\n\n```bash\nllama-server \\\n  --model ~/.cache/llama.cpp/Qwen3-30B-A3B-Thinking-2507-IQ4_XS.gguf \\\n  --n-gpu-layers 20 \\\n  --host 0.0.0.0 \\\n  --port 8080 \\\n  --ctx-size 4096\n```\n\n### Managing multiple models\n\nHopefully, you were able to chat with the model. Here is the problem that we will try to solve in this section.\n\nAssume that you have downloaded a GGUF file for another model. If you want to use this new model, you have to stop the old running server and start it again to host the new model.\n\nDoing this manually every time you want to change models is repetitive and not fun at all. We donâ€™t have this issue with ollama, we can download as many models as we like.\n\nThen, we add them to the list of models in `librechat.yaml` and restart the application. We can start a conversation with one model, switch to another and behind the scenes ollama will handle running the models for you.\n\nLuckily, we have [llama-swap](https://github.com/mostlygeek/llama-swap). This tool acts as a smart proxy that sits between LibreChat and `llama.cpp`. When you select a model in the LibreChat UI, `llama-swap` intercepts the request and automatically starts the correct `llama-server` for the model you chose after stopping any other that might be running.\n\nLetâ€™s set up the `llama-swap` tool. Go to the [llama-swap releases page](https://github.com/mostlygeek/llama-swap/releases) and download the archive that matches your operating system and CPU architecture. For example, I chose `llama-swap_162_linux_amd64.tar.gz` because I am on Ubuntu with a 64-bit Intel CPU.\n\n_The releases page of the llama-swap project._\n\nOpen your terminal, navigate to your Downloads folder, and extract the `llama-swap` executable using the `tar` command.\n\n```bash\n# Make sure to use the exact filename you downloaded\ntar -xvzf llama-swap_162_linux_amd64.tar.gz\n```\n\nFor better organization, create a dedicated folder for `llama-swap` and move the executable into it.\n\n```bash\nmkdir ~/llama-swap\nmv ~/Downloads/llama-swap ~/llama-swap/\n```\n\n`llama-swap` uses a single `config.yaml` file to know which models you have and how to run them.\n\n- Inside your `~/llama-swap` directory, create a new file named `config.yaml`.\n- Add your models to the file. For each one, provide a name and the exact `llama-server` command needed to run it.\n- `llama-swap` requires you to use `${PORT}` in your command. It will automatically assign a free port when it starts a server.\n- The `ttl` value tells `llama-swap` to shut down an inactive model server to free up memory.\n\nHere is an example configuration. Modify it to match the models you have downloaded.\n\n```yaml\nmodels:\n  \"gemma-3-1b-it\":\n    cmd: |\n      llama-server\n      --model /home/imad-saddik/.cache/llama.cpp/ggml-org_gemma-3-1b-it-GGUF_gemma-3-1b-it-Q4_K_M.gguf\n      --n-gpu-layers 999\n      --ctx-size 8192\n      --port ${PORT}\n    ttl: 300 # 5 minutes\n\n  \"Qwen3-30B-A3B-Thinking\":\n    cmd: |\n      llama-server\n      --model /home/imad-saddik/.cache/llama.cpp/Qwen3-30B-A3B-Thinking-2507-IQ4_XS.gguf\n      --n-gpu-layers 20\n      --ctx-size 4096\n      --port ${PORT}\n    ttl: 300 # 5 minutes\n```\n\nLetâ€™s run the `llama-swap` server. This server will now act as the manager for all your models.\n\n```bash\n./llama-swap --listen 0.0.0.0:8080\n```\n\nGo back to your `LibreChat` project directory and open `librechat.yaml`. Update the `models.default` list to include the exact names of the models you just defined in `llama-swap`'s `config.yaml`.\n\n```yaml\nendpoints:\n  custom:\n    - name: \"llama.cpp\"\n      apiKey: \"llama-cpp-is-awesome\"\n      baseURL: \"[http://host.docker.internal:8080/v1](http://host.docker.internal:8080/v1)\"\n      models:\n        default: [\n            # These names MUST EXACTLY match the names in the llama-swap config.yaml\n            \"gemma-3-1b-it\",\n            \"Qwen3-30B-A3B-Thinking\",\n          ]\n        fetch: false\n      titleConvo: true\n      titleModel: \"current_model\"\n      modelDisplayLabel: \"Llama.cpp\"\n```\n\nApply the new configuration by restarting the LibreChat containers.\n\n```bash\ndocker compose restart\n```\n\nNavigate back to LibreChat at [http://localhost:3080](https://www.google.com/search?q=http://localhost:3080). When you click the model selector, you should now see a dropdown list with all the models you configured.\n\n_The new models appear in LibreChat._\n\nIn LibreChat, select one of your models, send a message, and wait for the reply. Open a new browser tab and navigate to the address of your `llama-swap` server: [http://localhost:8080](https://www.google.com/search?q=http://localhost:8080).\n\nThis is a web application that ships with `llama-swap`. it shows you logs, available models, the state of each server, and more.\n\n_The llama-swap web interface._\n\nClick on the [models tab](https://www.google.com/search?q=http://localhost:8080/ui/models) and make sure that the model that you are chatting with in LibreChat is in the ready state.\n\n_The model you are chatting with in LibreChat is in the ready state._\n\nThen, select your second model and send another message. Switch back to the `llama-swap` web interface and notice how the state has changed. The previous model is in the stopped state, while the new one is in the ready state.\n\n_The previous model is in the stopped state after switching to the other model._\n\nWhen you switch models in LibreChat, you will see `llama-swap` automatically stop the old server and start the new one. The logs will show a cleanup message like this.\n\n```text\nsrv    operator(): operator(): cleaning up before exit...\n```\n\nIf you stop interacting with a model, its status will remain `ready` for the duration you set in the `ttl` field. After that, `llama-swap` will automatically unload it to free up VRAM, and its status will change to `stopped`.\n\n_The modelâ€™s state changed to stopped because it was not used._\n\nThe corresponding log message will look like this.\n\n```text\n[INFO] <gemma-3-1b-it> Unloading model, TTL of 300s reached\nsrv    operator(): operator(): cleaning up before exit...\n```\n\nTo avoid starting `llama-swap` manually every time you reboot, you can set it up as a `systemd` service that runs automatically in the background. Since `llama-swap` is lightweight and only loads models when needed, itâ€™s fine to keep it running this way.\n\nCreate a new service file:\n\n```bash\nsudo nano /etc/systemd/system/llama-swap.service\n```\n\nPaste the following text into the editor. You **must** replace the placeholder values for `User`, `Group`, and the file paths.\n\n- To find your `User`, run `whoami`.\n- To find your `Group`, run `id`. Look for `gid=1000(group_name)`, `group_name` is your group.\n\n```ini\n[Unit]\nDescription=Llama Swap Proxy Server\nAfter=network.target\n\n[Service]\nUser=your_user # IMPORTANT: Change this\nGroup=your_group # IMPORTANT: Change this\nWorkingDirectory=/home/your_user/llama-swap # IMPORTANT: Change this\nExecStart=/home/your_user/llama-swap/llama-swap --listen 0.0.0.0:8080 # IMPORTANT: Change this\nRestart=always\nRestartSec=3\n\n[Install]\nWantedBy=multi-user.target\n```\n\nSave the file, exit the editor, and run these commands to activate and start your new service:\n\n```bash\nsudo systemctl daemon-reload\nsudo systemctl enable llama-swap\nsudo systemctl start llama-swap\n```\n\nTo view the logs for the running service, you can use the `journalctl` command.\n\n```bash\nsudo journalctl -f -u llama-swap.service\n```\n\nWith this setup, you donâ€™t need to manually stop services to manage memory. `llama-swap` handles it all for you.\n\nIsnâ€™t this awesome? I love llama-swap ðŸ¤\n\n## Working with embedding models\n\nSo far, weâ€™ve focused on chat models. Now, letâ€™s explore how to use an **embedding model**, which is designed to convert text into [dense vectors](https://www.pinecone.io/learn/series/nlp/dense-vector-embeddings-nlp/) for tasks like [Retrieval-Augmented Generation](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) (RAG) and [semantic search](https://en.wikipedia.org/wiki/Semantic_search).\n\nWeâ€™ll use the [Qwen3-Embedding-8B-Q5_K_M](https://huggingface.co/Qwen/Qwen3-Embedding-8B-GGUF) model, which is currently the top-performing embedding model on the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n\n_The MTEB leaderboard._\n\nYou can generate an embedding directly from the terminal using the `llama-embedding` program.\n\nFirst, download a GGUF version of the model. I chose the `Q5_K_M` quant because it offers a great balance of quality and performance, allowing me to offload the entire model to my GPU.\n\nHere is the command to generate an embedding for a piece of text:\n\n```bash\nllama-embedding \\\n  --model ~/.cache/llama.cpp/Qwen3-Embedding-8B-Q5_K_M.gguf \\\n  --n-gpu-layers 999 \\\n  --pooling last \\\n  --ubatch-size 1024 \\\n  --prompt \"You like embeddings?\"\n```\n\nYouâ€™ll notice I set `--n-gpu-layers` to `999`. This is a useful trick to tell `llama.cpp` to offload as many layers as possible to the GPU.\n\nIf this command gives you an **Out Of Memory (OOM)** error, look at the startup logs for lines that tell you the total number of layers in the model.\n\n```text\nload_tensors: offloading 36 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 37/37 layers to GPU\n```\n\nThis model has 37 layers. If you get an error, reduce the `--n-gpu-layers` value from the total, start with `36` and keep decreasing it until the error disappears. Also, be aware that increasing `--ctx-size` will consume more VRAM.\n\nWhile the `Qwen3-Embedding-8B` model supports a massive context window of up to **32,768 tokens**, it's more practical to work with smaller text chunks. For a good balance of context and performance, I recommend keeping your chunks between **1024** and **2048** tokens.\n\n### Serving the embedding model as an API\n\nTo use the embedding model as a service, run `llama-server` with the `--embedding` flag:\n\n```bash\nllama-server \\\n  --model ~/.cache/llama.cpp/Qwen3-Embedding-8B-Q5_K_M.gguf \\\n  --n-gpu-layers 999 \\\n  --ubatch-size 1024 \\\n  --host 0.0.0.0 \\\n  --port 8080 \\\n  --embedding \\\n  --pooling last\n```\n\nThis starts a server that exposes the OpenAI-compatible `/v1/embeddings` endpoint.\n\n### Connect to the server with Python\n\nYou can get embeddings from the server by sending a POST request. The payload is a JSON object containing the `input` text. The server will respond with the dense vector.\n\nOn my system, this request takes only **~30ms** which is really fast!\n\n```python\nimport json\n\nimport requests\n\ntry:\n    data = {\n        \"input\": \"What is the capital of Morocco?\",\n    }\n    response = requests.post(\n        url=\"http://localhost:8080/v1/embeddings\",\n        headers={\n            \"Content-Type\": \"application/json\",\n        },\n        data=json.dumps(data),\n    )\n    response.raise_for_status()\n\n    response_json = response.json()\n    embedding_vector = response_json[\"data\"][0][\"embedding\"]\n    print(f\"Size of embedding vector: {len(embedding_vector)}\")\n\nexcept requests.exceptions.RequestException as e:\n    print(f\"An error occurred: {e}\")\n```\n\n## Going multimodal\n\n### Working with vision models\n\nSo far, we have only worked with text. Now, letâ€™s explore **multimodal models** like [gemma-3â€“4b-it-GGUF](https://huggingface.co/unsloth/gemma-3-4b-it-GGUF) or [Qwen2.5-VL-3B-Instruct-GGUF](https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-GGUF) that can process and understand both text and images as input.\n\nRunning a multimodal model in `llama.cpp` involves two separate files that work together to answer your questions:\n\n- The language model **(**`.gguf`**)**: This is the standard model file we've been using. It understands language, performs reasoning, and generates the final text response.\n- The multimodal projector **(**`mmproj.gguf`**)**: This is a specialized model. Its job is to look at an image, process it, and translate what it sees into embeddings that the language model can understand.\n\n_The mmproj and language models work together to handle multimodal input._\n\n> [!NOTE]\n> The diagram above is a conceptual illustration I made to help explain the process. It does not reflect the exact internal architecture of any specific model.\n\nWhen downloading multimodal models from Hugging Face, you must make sure you get **both** the main `.gguf` file and the corresponding `mmproj.gguf` file.\n\n_Arrows point to the different files that you should be looking for when trying to download multimodal models._\n\n#### Running the model in the terminal\n\n`llama.cpp` provides a dedicated command-line tool, `llama-mtmd-cli`, for multimodal chat.\n\nFirst, letâ€™s make the tool easily accessible from anywhere by copying it to a system path.\n\n```bash\nsudo cp build/bin/llama-mtmd-cli /usr/local/bin/\n```\n\nNow, letâ€™s run the `gemma-3-4b-it-GGUF` model. We can use the `-hf` flag to automatically download the correct model and projector files from Hugging Face.\n\n```bash\nllama-mtmd-cli -hf ggml-org/gemma-3-4b-it-GGUF\n```\n\nOnce the model loads, you'll enter an interactive chat mode.\n\n```text\nRunning in chat mode, available commands:\n/image <path>    load an image\n/clear           clear the chat history\n/quit or /exit   exit the program\n```\n\nFirst, send a text message to make sure itâ€™s working. Then, use the `/image` command to load your image, followed by a question about it. I will give the model the first thumbnail that I designed for this article.\n\n```text\n> /image /home/imad-saddik/Downloads/thumbnail.png\n> /home/imad-saddik/Downloads/thumbnail.png image loaded\n```\n\nThe image is loaded, now letâ€™s ask the model about it.\n\n```text\n> What do you see in this image?\nencoding image slice...\nimage slice encoded in 709 ms\ndecoding image batch 1/1, n_tokens_batch = 256\nimage decoded (batch 1/1) in 11 ms\n\nHere's a breakdown of what I see in the image:\n\n* **Text:** The main text reads \"First experience w/ Llama.cpp\". \"w/\" is a shortened form of \"with\".\n* **C++ Logo:** There's a bright orange logo of a llama with a plus sign (+), representing the C++ programming language.\n* **Background:** The background is a gradient, transitioning from white on the left to black on the right.\n\n**Overall Impression:** The image appears to be a graphic related to a beginner's introduction to C++ programming, specifically using the Llama.cpp project (likely a popular tool for running large language models).\n\n>\n```\n\nNot bad! The background is solid white rather than a gradient, but hey it worked.\n\n#### Running the model in LibreChat\n\nYou can also add your vision model to your `llama-swap` setup to use it within LibreChat's graphical interface.\n\nAdd a new entry for the multimodal model in `config.yaml`. The important distinction is that you must include the `--mmproj` flag in the command, pointing to the projector file.\n\n```yaml\nmodels:\n  # ... your other models\n\n  \"gemma-3-4b-it\":\n    cmd: |\n      llama-server \\\n      --model /home/imad-saddik/.cache/llama.cpp/ggml-org_gemma-3-4b-it-GGUF_gemma-3-4b-it-Q4_K_M.gguf \\\n      --mmproj /home/imad-saddik/.cache/llama.cpp/ggml-org_gemma-3-4b-it-GGUF_mmproj-model-f16.gguf \\\n      --n-gpu-layers 999 \\\n      --ctx-size 8192 \\\n      --port ${PORT}\n    ttl: 300\n```\n\nApply the changes by restarting the service.\n\n```bash\nsudo systemctl restart llama-swap\n```\n\nAdd the new model name to your LibreChat configuration so it appears in the UI.\n\n```yaml\nendpoints:\n  custom:\n    - name: \"llama.cpp\"\n      apiKey: \"llama-cpp-is-awesome\"\n      baseURL: \"[http://host.docker.internal:8080/v1](http://host.docker.internal:8080/v1)\"\n      models:\n        default: [\n            # These names MUST EXACTLY match the names in the llama-swap config.yaml\n            \"gemma-3-1b-it\",\n            \"gemma-3-4b-it\", # Add the new model name\n            \"Qwen3-30B-A3B-Thinking\",\n          ]\n        fetch: false\n      titleConvo: true\n      titleModel: \"current_model\"\n      modelDisplayLabel: \"Canis majoris\"\n```\n\nRestart LibreChat so that it can load the new configuration.\n\n```bash\ndocker compose restart\n```\n\nIn LibreChat, load the `gemma-3-4b-it` model and send a text message to verify that the changes have taken effect.\n\nYou got a response back? Perfect. Now click on the attach files icon and upload an image.\n\n_Uploading an image in LibreChat._\n\nSend the image together with a text message. You should get a response confirming that the model understood the image.\n\n_Successfully used an image as input in LibreChat._\n\n#### Using the model in Python\n\nThis time instead of running the `llama-server` command to create a server that the Python script will connect to, we will use the `llama-swap` server since it is already configured to run automatically.\n\nThe OpenAI API expects multimodal input in a specific JSON format. The image is not sent as a file but as a [Base64-encoded data URI](https://en.wikipedia.org/wiki/Data_URI_scheme) inside the `messages` payload.\n\n- The `encode_image_to_data_uri` function handles reading the image file and encoding it into this required format.\n- The `get_vision_completion` function constructs the special `messages` list containing separate objects for the `text` and the `image_url`. It then sends this payload to `llama-swap`, which routes the request to the correct model.\n\n```python\nimport base64\nimport json\nimport mimetypes\n\nimport requests\n\nLLAMA_SWAP_URL = \"http://localhost:8080\"\n\n\ndef encode_image_to_data_uri(image_path: str) -> str:\n    mime_type, _ = mimetypes.guess_type(image_path)\n    if mime_type is None:\n        mime_type = \"application/octet-stream\"\n\n    with open(image_path, \"rb\") as image_file:\n        encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n    return f\"data:{mime_type};base64,{encoded_string}\"\n\n\ndef get_vision_completion(model_name: str, prompt: str, image_path: str) -> None:\n    try:\n        image_data_uri = encode_image_to_data_uri(image_path)\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": prompt},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_data_uri}},\n                ],\n            }\n        ]\n\n        data = {\n            \"messages\": messages,\n            \"model\": model_name,\n            \"max_tokens\": 1024,\n        }\n        response = requests.post(\n            url=f\"{LLAMA_SWAP_URL}/v1/chat/completions\",\n            headers={\"Content-Type\": \"application/json\"},\n            data=json.dumps(data),\n        )\n        response.raise_for_status()\n        response_json = response.json()\n\n        assistant_message = response_json[\"choices\"][0][\"message\"][\"content\"]\n        print(assistant_message.strip())\n\n    except requests.exceptions.RequestException as e:\n        print(f\"An error occurred: {e}\")\n\n\nmodel_to_use = \"gemma-3-4b-it\"  # This tells llama-swap to load the gemma-3-4b-it model\npath_to_image = \"./thumbnail.png\"  # Change the path\ntext_prompt = \"What do you see in the image?\"\n\nget_vision_completion(model_to_use, text_prompt, path_to_image)\n```\n\n### Working with audio models\n\nNext, weâ€™ll explore models that can understand audio. Audio models come in two main types:\n\n- Models built for specific tasks like [Automatic Speech Recognition](https://developer.nvidia.com/blog/essential-guide-to-automatic-speech-recognition-technology/) (ASR) or [Text-to-Speech](https://www.ibm.com/think/topics/text-to-speech) (TTS).\n- General models that combine audio understanding with language to reason about and discuss the content of an audio file.\n\n`llama.cpp` handles the second category of models like [Voxtral-Mini-3B-2507-GGUF](https://huggingface.co/ggml-org/Voxtral-Mini-3B-2507-GGUF) with the same two-file approach we saw with vision models: a main language model and a specialized audio projector.\n\n#### Running the model in the terminal\n\nLetâ€™s use `llama-mtmd-cli` to download and run [Voxtral-Mini-3B-2507-GGUF](https://huggingface.co/ggml-org/Voxtral-Mini-3B-2507-GGUF) for a quick test.\n\n```bash\nllama-mtmd-cli -hf ggml-org/Voxtral-Mini-3B-2507-GGUF\n```\n\nAfter it loads, you can use the `/audio` command to provide an audio file and then ask the model to transcribe it. I gave it an audio clip from one of my YouTube videos.\n\n```text\nRunning in chat mode, available commands:\n/audio <path>    load an audio\n/clear           clear the chat history\n/quit or /exit   exit the program\n\n> /audio /home/imad-saddik/Downloads/audio.mp3\n/home/imad-saddik/Downloads/audio.mp3 audio loaded\n\n> Please transcribe the following audio:\nencoding audio slice...\naudio slice encoded in 248 ms\ndecoding audio batch 1/1, n_tokens_batch = 187\naudio decoded (batch 1/1) in 7 ms\nencoding audio slice...\naudio slice encoded in 219 ms\ndecoding audio batch 1/1, n_tokens_batch = 187\naudio decoded (batch 1/1) in 3 ms\nencoding audio slice...\naudio slice encoded in 217 ms\ndecoding audio batch 1/1, n_tokens_batch = 187\naudio decoded (batch 1/1) in 3 ms\n\nHello everyone, welcome to this course about routing and pathfinding problems. We will focus on using the OSRM project, which stands for Open Source Routing Machine. I will show you how to install the routing engine on your computer and use it as a server to make HTTP requests. You will learn how to use OSRM's services, such as finding the shortest path between two or more points, customizing the profile used by OSRM, it could be cars, bikes, or even pedestrians, solving the traveling salesman problem, and much more. I will also show you how to download a specific map area to limit the search space. In this course, I will use Python in the backend to interact with the OSRM engine server. However, you can use any programming language that makes HTTP requests. We will also visualize the data we get from the engine on a map to make the process more fun and easy to understand. This will help us confirm that the data from OSRM is accurate. I hope you are excited about this course. I will do my best to provide high-quality content. Before we start, the source will be available on GitHub. Let's get started.\n\n>\n```\n\nThe output is excellent, I am really impressed with this small model.\n\n#### Python and LibreChat integration roadblock\n\nUnfortunately, while `Voxtral` is amazing in the CLI, its advanced audio capabilities are **not yet exposed** through the `llama-server` API.\n\nMy attempts to use it with LibreChat or standard OpenAI audio endpoints (like `/v1/audio/transcriptions`) in Python failed with a `404 Not Found` error.\n\n```text\nAn error occurred: 404 Client Error: Not Found for url: http://localhost:8080/v1/audio/transcriptions\n```\n\nI'm sure this will be implemented in the future, but for now, we need to try something else like [whisper.cpp](https://github.com/ggml-org/whisper.cpp) or [faster-whisper](https://github.com/SYSTRAN/faster-whisper).\n\n#### Speech-to-Text with Whisper\n\nSince our goal is to get audio input working in LibreChat, letâ€™s focus on a dedicated Speech-to-Text (STT) model, [Whisper](https://openai.com/index/whisper/).\n\nThere are different ways to run Whisper. We can use `whisper.cpp`, `faster-whisper` or other projects.\n\nClone the `whisper.cpp` project.\n\n```bash\ngit clone [https://github.com/ggml-org/whisper.cpp.git](https://github.com/ggml-org/whisper.cpp.git)\ncd whisper.cpp\n```\n\n[Download the model from Hugging Face](https://huggingface.co/ggerganov/whisper.cpp). I picked the large version since it fits comfortably on my GPU.\n\n```bash\nwget -O ~/.cache/llama.cpp/whisper-large-v3-turbo-q8_0.gguf \"[https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large-v3-turbo-q8_0.bin](https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large-v3-turbo-q8_0.bin)\"\n```\n\nI will build the project with CUDA support. Check the projectâ€™s [README.md](https://github.com/ggml-org/whisper.cpp/blob/master/README.md) for other options.\n\n```bash\ncmake -B build -DGGML_CUDA=1\ncmake --build build -j --config Release\n```\n\nYou should see something like this in the end.\n\n```text\n[ 99%] Linking CXX executable ../../bin/whisper-cli\n[ 99%] Built target whisper-cli\n[100%] Linking CXX executable ../../bin/whisper-server\n[100%] Built target whisper-server\n```\n\nCopy the programs to `/usr/local/bin`.\n\n```bash\nsudo cp build/bin/whisper-server /usr/local/bin/\nsudo cp build/bin/whisper-cli /usr/local/bin/\n```\n\nNow, letâ€™s add our new `whisper-server` to `llama-swap`'s configuration. Add the following entry.\n\n```yaml\nmodels:\n  ...\n  \"whisper-large-v3-turbo\":\n    cmd: |\n      whisper-server\n      --model /home/imad-saddik/.cache/llama.cpp/whisper-large-v3-turbo-q8_0.gguf\n      --port ${PORT}\n      --host 0.0.0.0\n      --request-path /v1/audio/transcriptions\n      --inference-path \"\"\n    checkEndpoint: /v1/audio/transcriptions/\n    ttl: 300\n```\n\nThe `--request-path` tells `whisper-server` which API endpoint to use. Now, restart the service.\n\n```bash\nsudo systemctl restart llama-swap\n```\n\nBefore trying LibreChat, letâ€™s confirm the API is working with a Python script. This script sends a local audio file to our server.\n\n```python\nimport requests\n\ntry:\n    audio_file_path = \"./audio.mp3\"\n    with open(audio_file_path, \"rb\") as audio_file:\n        files = {\"file\": (audio_file_path.split(\"/\")[-1], audio_file, \"audio/mp3\")}\n\n        response = requests.post(\n            url=\"http://localhost:8080/v1/audio/transcriptions\",\n            data={\"model\": \"whisper-large-v3-turbo\"},\n            files=files,\n        )\n        response.raise_for_status()\n\n    response_json = response.json()\n    transcribed_text = response_json[\"text\"]\n    print(transcribed_text.strip())\n\nexcept requests.exceptions.RequestException as e:\n    print(f\"An error occurred: {e}\")\nexcept FileNotFoundError:\n    print(\"Error: The file was not found\")\n```\n\nRunning this script should successfully return the full transcribed text, proving our `whisper.cpp` server and `llama-swap` integration is working correctly. Here is the output:\n\n```text\nHello everyone, welcome to this course about routing and pathfinding problems. We will focus on using the OSRM project, which stands for Open Source Routing Machine. I will show you how to install the routing engine on your computer and use it as a server to make HTTP requests. You will learn how to use the OSRM's services, such as finding the shortest path between two or more points, customizing the profile used by OSRM. It could be cars, bikes, or even pedestrians, solving the traveling salesman problem, and much more. I will also show you how to download a specific map area to limit the search space. In this course, I will use Python in the backend to interact with the OSRM engine server. However, you can use any programming language that can make HTTP requests. We will also visualize the data we get from the engine on a map to make the process more fun and easy to understand. This will help us confirm that the data from OSRM is accurate. I hope you are excited about this course. I will do my best to provide high quality content. Before I forget, the source code will be available on GitHub. Let's get started.\n```\n\nTo get the timestamps, use this code instead. It tells the `whisper-server` to give us detailed data from the model. This is achieved by adding `response_format` and `timestamp_granularities` to the payload.\n\n```python\nimport datetime\n\nimport requests\n\ntry:\n    audio_file_path = \"./audio.mp3\"\n    with open(audio_file_path, \"rb\") as audio_file:\n        files = {\"file\": (audio_file_path.split(\"/\")[-1], audio_file, \"audio/mp3\")}\n\n        response = requests.post(\n            url=\"http://localhost:8080/v1/audio/transcriptions\",\n            data={\n                \"model\": \"whisper-large-v3-turbo\",\n                \"response_format\": \"verbose_json\",\n                \"timestamp_granularities\": [\"segment\"],\n            },\n            files=files,\n        )\n        response.raise_for_status()\n\n    response_json = response.json()\n    for segment in response_json[\"segments\"]:\n        start_time_seconds = segment[\"start\"]\n        end_time_seconds = segment[\"end\"]\n        text = segment[\"text\"]\n\n        start_timestamp = str(datetime.timedelta(seconds=start_time_seconds))\n        end_timestamp = str(datetime.timedelta(seconds=end_time_seconds))\n\n        print(f\"[{start_timestamp} --> {end_timestamp}] {text.strip()}\")\n\nexcept requests.exceptions.RequestException as e:\n    print(f\"An error occurred: {e}\")\nexcept FileNotFoundError:\n    print(\"Error: The file was not found.\")\n```\n\nHere is the output of the script.\n\n```text\n[0:00:00 --> 0:00:04.240000] Hello, everyone. Welcome to this course about routing and\n[0:00:04.240000 --> 0:00:09.560000] pathfinding problems. We will focus on using the OSRM\n[0:00:09.560000 --> 0:00:14.170000] project, which stands for Open Source Routing Machine. I\n[0:00:14.170000 --> 0:00:17.520000] will show you how to install the routing engine on\n[0:00:17.520000 --> 0:00:22.640000] your computer and use it as a server to make HTTP requests.\n[0:00:22.640000 --> 0:00:26.080000] You will learn how to use the OSRM's services,\n[0:00:26.080000 --> 0:00:29.740000] such as finding the shortest path between two or more\n[0:00:29.740000 --> 0:00:33.600000] points, customizing the profile used by OSRM.\n[0:00:33.600000 --> 0:00:38.340000] It could be cars, bikes, or even pedestrians, solving the\n[0:00:38.340000 --> 0:00:42.720000] traveling salesman problem, and much more. I will also\n[0:00:42.720000 --> 0:00:47.450000] show you how to download a specific map area to limit the\n[0:00:47.450000 --> 0:00:51.760000] search space. In this course, I will use Python\n[0:00:51.760000 --> 0:00:56.010000] in the backend to interact with the OSRM engine server.\n[0:00:56.010000 --> 0:00:59.600000] However, you can use any programming language\n[0:00:59.600000 --> 0:01:03.940000] that can make HTTP requests. We will also visualize the\n[0:01:03.940000 --> 0:01:07.360000] data we get from the engine on a map to make the\n[0:01:07.360000 --> 0:01:11.970000] process more fun and easy to understand. This will help us\n[0:01:11.970000 --> 0:01:15.280000] confirm that the data from OSRM is accurate.\n[0:01:15.280000 --> 0:01:18.790000] I hope you are excited about this course. I will do my best\n[0:01:18.790000 --> 0:01:21.440000] to provide high quality content. Before I\n[0:01:21.440000 --> 0:01:24.940000] forget, the source code will be available on GitHub. Let's\n[0:01:24.940000 --> 0:01:25.680000] get started.\n```\n\nNow, letâ€™s use whisper in LibreChat. Open `librechat.yaml` and update the `speech` section.\n\n```yaml\nspeech:\n  speechTab:\n    speechToText:\n      engineSTT: \"external\"\n  stt:\n    openai:\n      url: \"[http://host.docker.internal:8080/v1/audio/transcriptions](http://host.docker.internal:8080/v1/audio/transcriptions)\"\n      apiKey: \"whisper-cpp-is-awesome\"\n      model: \"whisper-large-v3-turbo\" # The same name in config.yaml\n```\n\nRestart the LibreChat containers.\n\n```bash\ndocker compose restart\n```\n\nClick on the **Settings** button.\n\n_Steps to find the Settings button in LibreChat._\n\nClick on the **Speech** option, and make sure that the engine is set to `External` in the Speech to Text section.\n\n_The speech settings page._\n\nGo back and click on the microphone and start talking, when you are done click the button again to stop the recording. LibreChat will call the transcriptions endpoint and will send the data to whisper.\n\nGo to the `llama-swap` dashboard, you should see that whisper is starting to load.\n\n_llama-swap received the request and is starting to load whisper._\n\nAfter waiting for a few seconds, I got this error.\n\n_Error in LibreChat._\n\nLooking at the `llama-swap` logs reveals the problem.\n\n```text\nerror: failed to read audio data as wav (Unknown error)\nerror: failed to read audio data\n```\n\nThe issue is an **audio format incompatibility**. LibreChatâ€™s browser interface sends audio in `webm` format, but the default `whisper-server` expects `16-bit WAV`.\n\nThere is a section in the [README.md](https://github.com/ggml-org/whisper.cpp?tab=readme-ov-file#ffmpeg-support-linux-only) file that shows how to compile `whisper.cpp` with [ffmpeg](https://github.com/FFmpeg/FFmpeg) so that the program can handle different audio formats.\n\nI followed those instructions but couldnâ€™t solve the problem. Instead, I ran into new issues. The `whisper-cli` works fine with different audio files, but the `whisper-server` isnâ€™t functioning properly. I get errors like these.\n\n```text\nCouldn't open input file \u001aEß£\nerror: failed to ffmpeg decode '\u001aEß£'\nerror: failed to read audio data\n\nCouldn't open input file ID3\u0004\nerror: failed to ffmpeg decode 'ID3\u0004'\nerror: failed to read audio data\n```\n\nHopefully, this issue will be fixed soon. I didnâ€™t want to wait, so I found another project called [faster-whisper](https://github.com/SYSTRAN/faster-whisper). Itâ€™s similar to `whisper.cpp` but faster, and it can be installed as a Python package.\n\nLetâ€™s create a dedicated Python environment to keep our dependencies clean.\n\n```bash\n# Using Conda\nconda create -n faster_whisper python=3.12 -y\nconda activate faster_whisper\n\n# Or using Python's venv\npython -m venv .faster_whisper\nsource .faster_whisper/bin/activate\n```\n\nInstall `faster-whisper` and other libraries that we will use to create a server in Python.\n\n```bash\npip install faster-whisper\npip install fastapi \"uvicorn[standard]\" python-multipart\n```\n\nTo run the model on a GPU, follow the [instructions in the](https://github.com/SYSTRAN/faster-whisper?tab=readme-ov-file#gpu) README. Youâ€™ll need to install `cuBLAS` and `cuDNN`, either system-wide or inside a virtual environment.\n\nI chose to install them in a virtual environment, which makes it easier to remove them later if needed. You can install `cuBLAS` and `cuDNN` with the following command.\n\n```bash\npip install nvidia-cublas-cu12 \"nvidia-cudnn-cu12==9.*\"\n```\n\nYou must tell your system where to find the NVIDIA libraries you just installed by setting the `LD_LIBRARY_PATH` environment variable. If you donâ€™t do that, you will get this error.\n\n```text\nUnable to load any of {libcudnn_ops.so.9.1.0, libcudnn_ops.so.9.1, libcudnn_ops.so.9, libcudnn_ops.so}\nInvalid handle. Cannot load symbol cudnnCreateTensorDescriptor\nAborted (core dumped)\n```\n\nRun these commands in the terminal to fix the error.\n\n```bash\nCONDA_ENV_PATH=\"/home/imad-saddik/anaconda3/envs/faster_whisper\" # IMPORTANT: Change this\nSITE_PACKAGES_PATH=\"$CONDA_ENV_PATH/lib/python3.12/site-packages\" # IMPORTANT: Change this\n\nexport LD_LIBRARY_PATH=\"$SITE_PACKAGES_PATH/nvidia/cudnn/lib:$SITE_PACKAGES_PATH/nvidia/cublas/lib\"\"\n```\n\nI will use the `large-v3` model. If you canâ€™t run it, change the model name to a smaller version.\n\nIn the `WhisperModel` class, Iâ€™m using `cuda` as the device and `int8_float16` as the compute type. If you donâ€™t have a GPU, set the device to CPU like this.\n\n```python\nmodel = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n```\n\nAfter that, pass the audio to the `transcribe` method. It returns a generator, which allows you to loop through the segments as they are produced.\n\n```python\nfrom faster_whisper import WhisperModel\n\ntry:\n    model_size = \"large-v3\"\n    model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n\n    segments, info = model.transcribe(\"./audio.mp3\", beam_size=5)\n    print(\n        \"Detected language '%s' with probability %f\"\n        % (info.language, info.language_probability)\n    )\n\n    for segment in segments:\n        print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n\nexcept Exception as e:\n    print(\"Error during transcription:\", str(e))\n```\n\nIf you were able to run this code without any issue, you can create the `FastAPI` server. Start by creating an instance of the `FastAPI` class and adding a `/health` endpoint. This endpoint is important because `llama-swap` will use it to check whether the server is alive.\n\n```python\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"ok\"}\n```\n\nNext, set up logging and load the model at startup so it can be reused for every request.\n\n```python\nimport logging\n\nfrom fastapi import FastAPI\nfrom faster_whisper import WhisperModel\n\nlogging.basicConfig()\nlogger = logging.getLogger(__name__)\nlogging.getLogger(\"faster_whisper\").setLevel(logging.INFO)\n\nlogger.info(\"Loading the Whisper model\")\nwhisper_model = WhisperModel(\n    model_size_or_path=\"large-v3\",\n    device=\"cuda\",\n    compute_type=\"float16\"\n)\nlogger.info(\"Whisper model loaded successfully.\")\n\napp = FastAPI()\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"ok\"}\n```\n\nFinally, add the `/v1/audio/transcriptions` endpoint. In the `transcribe_audio` method, a temporary file is created with the audio data received from LibreChat. The file is then passed to the `transcribe` method, the segments are combined, and the final text is returned to the frontend.\n\n```python\nimport logging\nimport os\nimport tempfile\n\nfrom fastapi import FastAPI, File, Form, UploadFile\nfrom faster_whisper import WhisperModel\n\nlogging.basicConfig()\nlogger = logging.getLogger(__name__)\nlogging.getLogger(\"faster_whisper\").setLevel(logging.INFO)\n\nlogger.info(\"Loading the Whisper model\")\nwhisper_model = WhisperModel(\n    model_size_or_path=\"large-v3\", device=\"cuda\", compute_type=\"float16\"\n)\nlogger.info(\"Whisper model loaded successfully.\")\n\napp = FastAPI()\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"ok\"}\n\n\n@app.post(\"/v1/audio/transcriptions\")\nasync def transcribe_audio(\n    file: UploadFile = File(...),\n    model: str = Form(...),\n):\n    logger.info(f\"Received request for model: {model}\")\n\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".tmp\") as tmp_file:\n        tmp_file.write(await file.read())\n        tmp_file_path = tmp_file.name\n\n    logger.info(f\"Transcribing audio file: {file.filename}\")\n    segments, info = whisper_model.transcribe(tmp_file_path, beam_size=5)\n    full_text = \"\".join(segment.text for segment in segments)\n    os.unlink(tmp_file_path)\n    logger.info(f\"Transcription successful. Detected language: {info.language}\")\n\n    return {\"text\": full_text.strip()}\n```\n\nNow weâ€™ll tell `llama-swap` how to manage our new FastAPI server. Create a bash script.\n\n```bash\nnano start.sh\n```\n\nThis script sets the required environment variable and then starts the [uvicorn](https://github.com/Kludex/uvicorn) server, passing along the port assigned by `llama-swap`.\n\n```bash\n#!/bin/bash\n\nPROJECT_DIR=\"/home/imad-saddik/Programs/faster-whisper\"\nCONDA_ENV_PATH=\"/home/imad-saddik/anaconda3/envs/faster_whisper\"\nSITE_PACKAGES_PATH=\"$CONDA_ENV_PATH/lib/python3.12/site-packages\"\n\nexport LD_LIBRARY_PATH=\"$SITE_PACKAGES_PATH/nvidia/cudnn/lib:$SITE_PACKAGES_PATH/nvidia/cublas/lib\"\n\ncd \"$PROJECT_DIR\"\n\"$CONDA_ENV_PATH/bin/uvicorn\" server:app --host 0.0.0.0 \"$@\"\n```\n\nMake the script executable.\n\n```bash\nchmod +x start.sh\n```\n\nNext, we need to update the `config.yaml` file. We will configure `llama-swap` to:\n\n- Run our bash script (`start.sh`).\n- Check the `/health` endpoint to verify that the FastAPI server is running.\n- After 300 seconds, use `pkill` to stop the FastAPI server.\n\n```yaml\nmodels:\n  ...\n  \"whisper-large-v3-turbo\":\n    cmd: /home/imad-saddik/Programs/faster-whisper/start.sh --port ${PORT}\n    cmdStop: pkill -f \"uvicorn server:app\"\n    proxy: [http://127.0.0.1](http://127.0.0.1):${PORT}\n    checkEndpoint: \"/health\"\n    ttl: 300\n```\n\nRestart the `llama-swap` service.\n\n```bash\nsudo systemctl restart llama-swap\n```\n\nIn LibreChat, click the microphone icon to start recording, speak, and click it again to stop. The icon will show a loading state while the FastAPI server starts.\n\nOpen the `llama-swap` interface.\n\n_The model state and logs in the llama-swap interface._\n\nYou should see that the model is in a **ready state**, and if you inspect the logs, you will see entries like the following.\n\n```text\nINFO: Started server process [133188]\nINFO: Waiting for application startup.\nINFO: Application startup complete.\nINFO: Uvicorn running on [http://0.0.0.0:10004](http://0.0.0.0:10004) (Press CTRL+C to quit)\nINFO: 127.0.0.1:49778 - \"GET /health HTTP/1.1\" 200 OK\nINFO:faster_whisper:Processing audio with duration 00:06.414\nINFO:faster_whisper:Detected language 'ar' with probability 0.33\nINFO: 127.0.0.1:49784 - \"POST /v1/audio/transcriptions HTTP/1.1\" 200 OK\n```\n\nI have to admit, I spent a lot of time on this feature, so Iâ€™m really happy that it worked in the end.\n\n## Optimizing performance with llama-bench\n\nManually tweaking arguments like `--n-gpu-layers` and `--ctx-size` to find the best performance for your hardware is not fun. `llama.cpp` includes a tool called `llama-bench` that automates this process by running a series of benchmarks and presenting the results in a clear table.\n\nMake the tool easily accessible from your terminal.\n\n```bash\nsudo cp build/bin/llama-bench /usr/local/bin/\n```\n\nYou can run `llama-bench -h` to see all available options. Read the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/tools/llama-bench/README.md) to find examples of how to use the tool.\n\n### Find the optimal number of GPU layers\n\nOur first goal is to find the number of GPU layers that gives the highest generation speed (tokens/second) without using all available VRAM.\n\nLet's benchmark the `Qwen3-30B-A3B-Thinking` model across a range of GPU layer counts.\n\n```bash\nllama-bench \\\n  --model ~/.cache/llama.cpp/Qwen3-30B-A3B-Thinking-2507-IQ4_XS.gguf \\\n  --n-gen 128 \\\n  --n-prompt 0 \\\n  --n-gpu-layers 18-24+1\n```\n\nThe argument `--n-gpu-layers 18-24+1` tells `llama-bench` to start with 18 layers and increase the count by 1 up to 24. In each run, the model will generate 128 tokens.\n\n_The results returned by llama-bench._\n\nThe table shows that offloading **19 layers** provides the highest generation speed. We can also see the final run for 24 layers failed due to insufficient VRAM.\n\n```text\nmain: error: failed to create context with model '/home/imad-saddik/.cache/llama.cpp/Qwen3-30B-A3B-Thinking-2507-IQ4_XS.gguf'\n```\n\n### Determine maximum context size and speed trade-offs\n\nNow that we know our optimal speed is at 19 GPU layers, letâ€™s see how performance changes as the context window fills up. We can use the `--n-depth` argument to benchmark the model at different context depths.\n\nThis simulates how it would perform with a pre-existing conversation history of `0`, `4096`, `8192`, and `16384` tokens.\n\n```bash\nllama-bench \\\n  --model ~/.cache/llama.cpp/Qwen3-30B-A3B-Thinking-2507-IQ4_XS.gguf \\\n  --n-gen 128 \\\n  --n-prompt 0 \\\n  --n-gpu-layers 19 \\\n  --n-depth 0,4096,8192,16384\n```\n\n_The results returned by llama-bench._\n\nThe results show a clear trade-off. With 19 layers on the GPU, we donâ€™t have enough leftover VRAM to handle a 16,384 token context. To support a larger context, we must sacrifice some speed by offloading fewer layers. Letâ€™s test that hypothesis by reducing the layers to 16.\n\n```bash\nllama-bench \\\n  --model ~/.cache/llama.cpp/Qwen3-30B-A3B-Thinking-2507-IQ4_XS.gguf \\\n  --n-gen 128 \\\n  --n-prompt 0 \\\n  --n-gpu-layers 16 \\\n  --n-depth 0,4096,8192,16384\n```\n\n_The results returned by llama-bench._\n\nBy offloading only 16 layers, we can now handle the 16K context window. Notice how the generation speed decreases when the context window increases. This is normal because the model needs to do a lot of work.\n\nFor most tasks, you wonâ€™t need a large context window. But if you plan to upload long documents, the window can fill up quickly. To avoid manually changing arguments each time, create two entries in `config.yaml`: one for small tasks, and another for tasks requiring a large context window.\n\n```yaml\nmodels:\n  \"Qwen3-30B-A3B-Thinking - 4K\":\n    cmd: |\n      llama-server\n      --model /home/imad-saddik/.cache/llama.cpp/Qwen3-30B-A3B-Thinking-2507-IQ4_XS.gguf\n      --n-gpu-layers 19\n      --ctx-size 4096\n      --port ${PORT}\n    ttl: 300\n\n  \"Qwen3-30B-A3B-Thinking - 16K\":\n    cmd: |\n      llama-server\n      --model /home/imad-saddik/.cache/llama.cpp/Qwen3-30B-A3B-Thinking-2507-IQ4_XS.gguf\n      --n-gpu-layers 16\n      --ctx-size 16384\n      --port ${PORT}\n    ttl: 300\n```\n\nRestart the `llama-swap` service.\n\n```bash\nsudo systemctl restart llama-swap\n```\n\nUpdate the `librechat.yaml` file to include the new models.\n\n```yaml\nendpoints:\n  custom:\n    - name: \"llama.cpp\"\n      apiKey: \"llama-cpp-is-awesome\"\n      baseURL: \"[http://host.docker.internal:8080/v1](http://host.docker.internal:8080/v1)\"\n      models:\n        default: [\n            # These names MUST EXACTLY match the names in the llama-swap config.yaml\n            \"gemma-3-1b-it\",\n            \"gemma-3-4b-it\",\n            \"Qwen3-30B-A3B-Thinking - 4K\",\n            \"Qwen3-30B-A3B-Thinking - 16K\",\n          ]\n        fetch: false\n      titleConvo: true\n      titleModel: \"current_model\"\n      modelDisplayLabel: \"Canis majoris\"\n```\n\nFinally, restart LibreChat.\n\n```bash\ndocker compose restart\n```\n\nThis is how you can use `llama-bench` to systematically find the best settings for your hardware. For more examples, please read the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/tools/llama-bench/README.md) file.\n\n## Automating the entire workflow\n\n### Creating desktop shortcuts for easy access\n\nTo make launching everything easier, weâ€™ll be accessing two web interfaces:\n\n- **llama-swap interface**: [http://localhost:8080/](https://www.google.com/search?q=http://localhost:8080/)\n- **LibreChat**: [http://localhost:3080/](https://www.google.com/search?q=http://localhost:3080/)\n\nInstead of remembering these links, we can create desktop shortcuts to open these links.\n\n#### llama-swap\n\nCreate a new launcher file in your local applications directory.\n\n```bash\nnano ~/.local/share/applications/llama-swap.desktop\n```\n\nPaste the following into the editor. This defines the launcherâ€™s name, action, and icon. **Remember to change the** `Icon` **path** to where you saved the icon. You can [download the icon from here](https://github.com/mostlygeek/llama-swap/blob/main/ui/public/favicon.svg).\n\n```ini\n[Desktop Entry]\nVersion=1.0\nType=Application\nName=Llama Swap UI\nComment=Monitor local llama.cpp models\nExec=xdg-open http://localhost:8080/\nIcon=/path/to/icon.svg # IMPORTANT: Change this path\nTerminal=false\nCategories=Network;\n```\n\nMake the file executable.\n\n```bash\nchmod +x ~/.local/share/applications/llama-swap.desktop\n```\n\nRun the following command to make your new shortcut discoverable by your systemâ€™s application menu.\n\n```bash\nupdate-desktop-database ~/.local/share/applications\n```\n\nSearch for **Llama Swap UI**. You should see it appear in the search result. Click it and verify that it opens the browser with the correct link.\n\n_The Llama Swap UI desktop shortcut._\n\n#### LibreChat\n\nCreate another launcher file in your local applications directory.\n\n```bash\nnano ~/.local/share/applications/librechat.desktop\n```\n\nPaste the following into the editor. **Remember to change the** `Exec` **and** `Icon` **paths**. You can [download the icon from here](https://github.com/danny-avila/LibreChat/blob/main/client/public/assets/logo.svg).\n\n```ini\n[Desktop Entry]\nVersion=1.0\nName=LibreChat\nComment=Run LibreChat with Docker\nExec=/path/to/LibreChat/start.sh # IMPORTANT: Change this path\nIcon=/path/to/logo.svg # IMPORTANT: Change this path\nTerminal=false\nType=Application\nCategories=Utility;Development;\n```\n\nMake the file executable.\n\n```bash\nchmod +x ~/.local/share/applications/librechat.desktop\n```\n\nNavigate to your `LibreChat` project directory and create a new script file.\n\n```bash\nnano start.sh\n```\n\nThis script will automatically navigate to the correct directory, run `docker compose up -d`, and launch Firefox. **Remember to change the** `DOCKER_COMPOSE_DIR` **path**.\n\n```bash\n#!/bin/bash\n\nDOCKER_COMPOSE_DIR=\"/path/to/LibreChat/\" # IMPORTANT: Change this path\ncd \"$DOCKER_COMPOSE_DIR\"\ndocker compose up -d\nfirefox http://localhost:3080\n```\n\nSearch for **LibreChat** in your application menu and click it. It should start the Docker containers (if they arenâ€™t already running) and open the UI in your browser.\n\n_The LibreChat desktop shortcut._\n\n### Automatically restarting services on configuration changes\n\nSo far, we have to manually restart `llama-swap` or `LibreChat` every time we edit their `.yaml` configuration files. This is repetitive and easy to forget. We can do better by creating a fully automated workflow that watches for changes and triggers restarts for us.\n\nWe will rely on [inotify-tools](https://github.com/inotify-tools/inotify-tools) for this automation. This is a powerful Linux utility that can monitor files for events like modifications.\n\nOn Ubuntu, you can install `inotify-tools` like so.\n\n```bash\nsudo apt update\nsudo apt install inotify-tools\n```\n\nFirst, weâ€™ll create a bash script that will contain the logic for watching the files and triggering the restarts.\n\n```bash\nsudo nano /usr/local/bin/config_watcher.sh\n```\n\nPaste the following code into the file.\n\n```bash\n#!/bin/bash\n\n# IMPORTANT: UPDATE THESE PATHS\nLLAMA_SWAP_CONFIG=\"/home/imad-saddik/Programs/llama-swap/config.yaml\"\nLIBRECHAT_CONFIG=\"/home/imad-saddik/snap/LibreChat/librechat.yaml\"\nLIBRECHAT_DIR=\"/home/imad-saddik/snap/LibreChat/\"\n\nwhile true; do\n  # Wait for a modification event on either file.\n  # The --format '%w%f' prints the full path of the modified file.\n  MODIFIED_FILE=$(inotifywait -q -e modify --format '%w%f' \"$LLAMA_SWAP_CONFIG\" \"$LIBRECHAT_CONFIG\")\n\n  if [ \"$MODIFIED_FILE\" == \"$LLAMA_SWAP_CONFIG\" ]; then\n    echo \"$(date): Change detected in $LLAMA_SWAP_CONFIG. Restarting all services...\"\n    sudo systemctl restart llama-swap\n    (cd \"$LIBRECHAT_DIR\" && docker compose restart)\n    echo \"$(date): All services restarted successfully.\"\n\n  elif [ \"$MODIFIED_FILE\" == \"$LIBRECHAT_CONFIG\" ]; then\n    echo \"$(date): Change detected in $LIBRECHAT_CONFIG. Restarting LibreChat containers...\"\n    (cd \"$LIBRECHAT_DIR\" && docker compose restart)\n    echo \"$(date): LibreChat containers restarted successfully.\"\n  fi\ndone\n```\n\nI begin the script by creating variables to improve code readability. Make sure to update the paths to the `config.yaml` file, the `librechat.yaml` file, and the LibreChat folder containing the source code.\n\nNext, I use an infinite loop to continuously monitor these files. The following line of code pauses execution and waits for any signal indicating that a file has changed.\n\n```bash\nMODIFIED_FILE=$(inotifywait -q -e modify --format '%w%f' \"$LLAMA_SWAP_CONFIG\" \"$LIBRECHAT_CONFIG\")\n```\n\nWe use the `inotifywait` command to monitor the two files: `config.yaml` and `librechat.yaml`. The file paths are provided at the end of the command as `$LLAMA_SWAP_CONFIG` and `$LIBRECHAT_CONFIG`.\n\nI included the `-q` option so that `inotifywait` runs quietly without printing any output, and the `-e modify` option to listen specifically for modifications. This event is triggered whenever a file is saved.\n\nThe `--format '%w%f'` option tells `inotifywait` to output the full path of the file that was changed. This path is then stored in the `MODIFIED_FILE` variable.\n\nOnce we have the path of the modified file, we check whether it is `config.yaml` or `librechat.yaml`. Inside the corresponding `if-else` blocks, we execute the commands that we previously ran manually.\n\n```bash\nsudo systemctl restart llama-swap\ndocker compose restart\n```\n\nNow, the script handles them automatically for us.\n\nNotice something: to restart the `llama-swap` service, we need to use `sudo`. This is a problem because the script will pause and wait for a password.\n\nFortunately, we can create a rule that allows this script to run without requiring a password. To do this, open the `sudoers` file for editing using `visudo`. Itâ€™s important to use `visudo` because it checks for syntax errors before saving.\n\n```bash\nsudo visudo\n```\n\nScroll to the bottom of the file and add the following line.\n\n```text\nimad-saddik ALL=(ALL) NOPASSWD: /usr/bin/systemctl restart llama-swap\n```\n\nHereâ€™s what each part means:\n\n- `imad-saddik` is the user who gets the permission. The rule applies to this user only.\n- `ALL=(ALL)` has two parts: the first `ALL=` means the rule applies to the current machine, and `(ALL)` means the command can be run as any user, granting root privileges.\n- `NOPASSWD` tells the system that when `imad-saddik` runs the `/usr/bin/systemctl restart llama-swap` command, no password is required. Without this, the script would stop every time it tried to run `sudo`, waiting for a password.\n- `/usr/bin/systemctl restart llama-swap` is the exact command that `imad-saddik` is allowed to run without a password. No other commands are affected.\n\nSave the file and exit. Next, make the script executable.\n\n```bash\nsudo chmod +x /usr/local/bin/config_watcher.sh\n```\n\nNow, create a systemd service to run the script automatically.\n\n```bash\nsudo nano /etc/systemd/system/config-watcher.service\n```\n\nCopy and paste the following configuration. This tells systemd to run the script under your user account.\n\n```ini\n[Unit]\nDescription=Watcher for llama-swap and librechat config files\nAfter=network.target\n\n[Service]\nUser=imad-saddik\nGroup=imad-saddik\nExecStart=/usr/local/bin/config_watcher.sh\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n```\n\nEnable and start the service.\n\n```bash\nsudo systemctl daemon-reload\nsudo systemctl enable config-watcher.service\nsudo systemctl start config-watcher.service\n```\n\nCheck the status to make sure it is running correctly.\n\n```bash\nsudo systemctl status config-watcher.service\n```\n\nYou should see output similar to this.\n\n```text\nâ— config-watcher.service - Watcher for llama-swap and librechat config files\n     Loaded: loaded (/etc/systemd/system/config-watcher.service; enabled; preset: enabled)\n     Active: active (running) since Wed 2025-10-01 21:19:38 +01; 2h 26min ago\n   Main PID: 23025 (config_watcher.)\n      Tasks: 2 (limit: 37817)\n     Memory: 1.7M (peak: 24.3M)\n        CPU: 140ms\n     CGroup: /system.slice/config-watcher.service\n             â”œâ”€23025 /bin/bash /usr/local/bin/config_watcher.sh\n             â””â”€28014 inotifywait -q -e modify --format %w%f /home/imad-saddik/Programs/llama-swap/config.yaml /home/i>\n```\n\nThis confirms that your script is now running as a background service and will automatically watch for changes to the configuration files.\n\nNow, make a change in `config.yaml` and save it. For example, remove a model from the list. In my case, I removed `Qwen3-30B-A3B-Thinking - 16K`:\n\n```yaml\nmodels:\n  \"gemma-3-1b-it\":\n    cmd: |\n      llama-server\n      --model /home/imad-saddik/.cache/llama.cpp/ggml-org_gemma-3-1b-it-GGUF_gemma-3-1b-it-Q4_K_M.gguf\n      --n-gpu-layers 999\n      --ctx-size 8192\n      --port ${PORT}\n    ttl: 300\n\n  \"Qwen3-30B-A3B-Thinking - 4K\":\n    cmd: |\n      llama-server\n      --model /home/imad-saddik/.cache/llama.cpp/Qwen3-30B-A3B-Thinking-2507-IQ4_XS.gguf\n      --n-gpu-layers 19\n      --ctx-size 4096\n      --port ${PORT}\n    ttl: 300\n\n  # \"Qwen3-30B-A3B-Thinking - 16K\":\n  #   cmd: |\n  #     llama-server\n  #     --model /home/imad-saddik/.cache/llama.cpp/Qwen3-30B-A3B-Thinking-2507-IQ4_XS.gguf\n  #     --n-gpu-layers 16\n  #     --ctx-size 16384\n  #     --port ${PORT}\n  #   ttl: 300\n\n  \"gemma-3-4b-it\":\n    cmd: |\n      llama-server\n      --model /home/imad-saddik/.cache/llama.cpp/ggml-org_gemma-3-4b-it-GGUF_gemma-3-4b-it-Q4_K_M.gguf\n      --mmproj /home/imad-saddik/.cache/llama.cpp/ggml-org_gemma-3-4b-it-GGUF_mmproj-model-f16.gguf\n      --n-gpu-layers 999\n      --ctx-size 8192\n      --port ${PORT}\n    ttl: 300\n\n  \"whisper-large-v3-turbo\":\n    cmd: /home/imad-saddik/Programs/faster-whisper/start.sh --port ${PORT}\n    cmdStop: pkill -f \"uvicorn server:app\"\n    proxy: [http://127.0.0.1](http://127.0.0.1):${PORT}\n    checkEndpoint: \"/health\"\n    ttl: 300\n```\n\nTo watch the logs in real time, run.\n\n```bash\njournalctl -u config-watcher.service -f\n```\n\nYou should see that the change is detected, for example.\n\n```text\n$ journalctl -u config-watcher.service -f\nOct 01 21:19:38 saddik systemd[1]: Started config-watcher.service - Watcher for llama-swap and librechat config files.\nOct 01 21:20:47 saddik config_watcher.sh[23025]: Wed Oct 1 09:20:47 PM +01 2025: **Change detected** in /home/imad-saddik/Programs/llama-swap/config.yaml. Restarting all services...\nOct 01 21:20:47 saddik sudo[23945]: imad-saddik : PWD=/ ; USER=root ; COMMAND=/usr/bin/systemctl restart llama-swap\n```\n\nTo verify that it worked, open the `llama-swap` interface and confirm that the removed model no longer appears. In this example, `Qwen3-30B-A3B-Thinking - 16K` is gone.\n\n_Qwen3-30B-A3B-Thinking â€” 16K is no longer visible._\n\nNext, do the same with `librechat.yaml` by updating the list of models.\n\n```yaml\nendpoints:\n  custom:\n    - name: \"llama.cpp\"\n      apiKey: \"llama-cpp-is-awesome\"\n      baseURL: \"[http://host.docker.internal:3647/v1](http://host.docker.internal:3647/v1)\"\n      models:\n        default: [\n            # These names MUST EXACTLY match the names in the llama-swap config.yaml\n            \"gemma-3-1b-it\",\n            \"gemma-3-4b-it\",\n            \"Qwen3-30B-A3B-Thinking - 4K\",\n            # \"Qwen3-30B-A3B-Thinking - 16K\",\n          ]\n```\n\nSave the file and watch the logs again. You should see something like.\n\n```text\nOct 01 21:21:39 saddik config_watcher.sh[23025]: Wed Oct 1 09:21:39 PM +01 2025: **Change detected** in /home/imad-saddik/snap/LibreChat/librechat.yaml. Restarting LibreChat containers...\n```\n\nOpen LibreChat and check the model list, `Qwen3-30B-A3B-Thinking - 16K` is no longer available.\n\n_The Qwen3-30B-A3B-Thinking â€” 16K can no longer be selected in LibreChat._\n\nNow you can enjoy watching the script automatically restart the services whenever you make changes. This feature saves time and makes sure you never forget to restart anything. I really like it, and I hope you do too!\n\n## Keep things up-to-date\n\nThe projects we are using are actively developed. It is a good idea to update them from time to time to get the latest features, bug fixes, and performance improvements.\n\n### Llama.cpp\n\nThe `llama.cpp` project moves very quickly. Here is how you can update it manually.\n\nFirst, navigate to your `llama.cpp` project directory.\n\n```bash\ncd /path/to/llama.cpp\n```\n\nPull the latest changes from the GitHub repository.\n\n```bash\ngit pull --rebase\n```\n\nNext, remove the old `build` directory to make sure you perform a clean compilation.\n\n```bash\nrm -rf build/\n```\n\nNow, recompile the project using the same commands you used during the initial installation. Remember to use the correct flags for your hardware (e.g., `GGML_CUDA=ON` for NVIDIA GPUs).\n\n```bash\ncmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=89\ncmake --build build --config Release -j $(nproc)\n```\n\nFinally, copy the newly compiled programs to `/usr/local/bin`, replacing the old versions.\n\n```bash\nsudo cp build/bin/llama-cli /usr/local/bin/\nsudo cp build/bin/llama-server /usr/local/bin/\nsudo cp build/bin/llama-embedding /usr/local/bin/\nsudo cp build/bin/llama-mtmd-cli /usr/local/bin/\nsudo cp build/bin/llama-bench /usr/local/bin/\n```\n\nThat was the manual approach. Now, letâ€™s look at how to automate these steps so they run periodically. You can automate this process using a [cron job](https://en.wikipedia.org/wiki/Cron).\n\nFirst, create a shell script that contains all the update commands.\n\n```bash\nsudo nano /usr/local/bin/update_llama_cpp.sh\n```\n\nPaste the following code into the script. **Make sure to change the** `LLAMA_CPP_DIR` **path** to the correct location on your system.\n\n```bash\n#!/bin/bash\n\nLLAMA_CPP_DIR=\"/path/to/your/llama.cpp\" # IMPORTANT: Change this path\ncd \"$LLAMA_CPP_DIR\" || exit\n\ngit pull --rebase\n\nrm -rf build/\n\ncmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=89\ncmake --build build --config Release -j $(nproc)\n\ncp build/bin/llama-cli /usr/local/bin/\ncp build/bin/llama-server /usr/local/bin/\ncp build/bin/llama-embedding /usr/local/bin/\ncp build/bin/llama-mtmd-cli /usr/local/bin/\ncp build/bin/llama-bench /usr/local/bin/\n\necho \"llama.cpp updated successfully on $(date)\"\n```\n\nMake the script executable.\n\n```bash\nsudo chmod +x /usr/local/bin/update_llama_cpp.sh\n```\n\nOpen your crontab file for editing.\n\n```bash\ncrontab -e\n```\n\nAdd the following line to the end of the file. This will run your update script **every day at 6:00 AM**.\n\n```text\n0 6 * * * sudo /home/your_user/update_llama_cpp.sh >> /home/your_user/llama_cpp_update.log 2>&1\n```\n\nLet me explain this line:\n\n- `0 6 * * *`: This is the schedule. It means \"at minute 0 of hour 6, every day, every month, every day of the week.\"\n- `/home/your_user/update_llama_cpp.sh`: This is the command to run. **Make sure to use the full path to your script**.\n- `>> /home/your_user/llama_cpp_update.log 2>&1`: This part redirects the output of the script to a log file.\n\nSave and close the file. Now your `llama.cpp` installation will be updated automatically every day.\n\nOpen the `sudoers` file.\n\n```bash\nsudo visudo\n```\n\nScroll to the bottom and add the following line to allow the script to run without requiring a password.\n\n```text\nimad-saddik ALL=(ALL) NOPASSWD: /usr/local/bin/update_llama_cpp.sh\n```\n\nI ran into a few issues with **crontab**. Sometimes it executed my job, and other times it didnâ€™t, even though my laptop was powered on at the scheduled time. To avoid dealing with inconsistent behavior, I switched to **anacron**.\n\nAnacron works differently from crontab. Instead of running a job at a specific hour, it only guarantees when the job should run: daily, weekly, monthly, and so on. If the system was off at the scheduled time, anacron will simply run the job the next time the machine is on. This is exactly what I needed.\n\nSince I want to update **llama.cpp** every day, I placed the update script directly inside the `/etc/cron.daily/` directory. Files inside this directory are executed once per day by anacron. Create the script as follows (without a file extension).\n\n```bash\nsudo nano /etc/cron.daily/update-llama-cpp\n```\n\nNow, paste this entire content into the editor.\n\n```bash\n#!/bin/bash\n\nLOG_DIR=\"/home/your_user/Programs/logs/llama.cpp\"\nLOG_FILE=\"$LOG_DIR/llama_cpp_update_$(date +%Y-%m-%d).log\"\nmkdir -p \"$LOG_DIR\"\n\n# This line redirects all output for the rest of the script\nexec >> \"$LOG_FILE\" 2>&1\n\necho \"--- Starting llama.cpp daily update at $(date) ---\"\nLLAMA_CPP_DIR=\"/home/your_user/Programs/llama.cpp\"\nif [ ! -d \"$LLAMA_CPP_DIR\" ]; then\n    echo \"Error: Directory $LLAMA_CPP_DIR does not exist.\"\n    exit 1\nfi\n\ncd \"$LLAMA_CPP_DIR\" || exit\n\necho \"Running git pull...\"\ngit pull --rebase\n\necho \"Cleaning build directory...\"\nrm -rf build/\n\necho \"Running cmake setup...\"\ncmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=89\n\necho \"Building project (using $(nproc) cores)...\"\ncmake --build build --config Release -j $(nproc)\n\necho \"Copying new binaries to /usr/local/bin/...\"\ncp build/bin/llama-cli /usr/local/bin/\ncp build/bin/llama-server /usr/local/bin/\ncp build/bin/llama-embedding /usr/local/bin/\ncp build/bin/llama-mtmd-cli /usr/local/bin/\ncp build/bin/llama-bench /usr/local/bin/\n\necho \"--- llama.cpp updated successfully on $(date) ---\"\nexit 0\n```\n\nSave the file and make it executable.\n\n```bash\nsudo chmod +x /etc/cron.daily/update-llama-cpp\n```\n\nAt this point, the old crontab entry is no longer needed. Open your crontab and delete the line you previously added.\n\n```bash\ncrontab -e\n```\n\nAnacron will automatically detect and run your new script during its next daily check. If you want to test it immediately, run it manually.\n\n```bash\nsudo /etc/cron.daily/update-llama-cpp\n```\n\nThen inspect the log file to confirm everything worked.\n\n```bash\ncat /home/imad-saddik/Programs/logs/llama.cpp/llama_cpp_update_$(date +%Y-%m-%d).log\n```\n\nFinally, the old script located at `/usr/local/bin/update_llama_cpp.sh` is no longer used by anything, so you can safely remove it.\n\n```bash\nsudo rm /usr/local/bin/update_llama_cpp.sh\n```\n\nThis guarantees that llama.cpp stays up-to-date every day without relying on crontabâ€™s unpredictable behavior.\n\n### llama-swap\n\nTo update `llama-swap`, visit the [releases page](https://github.com/mostlygeek/llama-swap/releases) on GitHub. Download the latest archive for your system, e.g. `llama-swap_XXX_linux_amd64.tar.gz`.\n\nExtract the archive and replace your old `llama-swap` executable with the new one.\n\n```bash\ntar -xvzf llama-swap_XXX_linux_amd64.tar.gz\nsudo mv llama-swap /path/to/your/old/llama-swap\n```\n\n### LibreChat\n\nUpdating LibreChat involves pulling the latest changes from GitHub and rebuilding the Docker containers.\n\nFirst, navigate to your LibreChat directory.\n\n```bash\ncd /path/to/LibreChat\n```\n\nPull the latest code changes from GitHub.\n\n```bash\ngit pull --rebase\n```\n\nStop and remove the current Docker containers.\n\n```bash\ndocker compose down\n```\n\nRemove the old Docker images to free up space.\n\n```bash\n# For Linux/Mac\ndocker images -a | grep \"librechat\" | awk '{print $3}' | xargs docker rmi\n```\n\nNow, pull the new application images that the updated configuration files refer to.\n\n```bash\ndocker compose pull\n```\n\nFinally, start the application again in detached mode.\n\n```bash\ndocker compose up -d\n```\n\n### whisper.cpp\n\nThe update process for `whisper.cpp` is the same as for `llama.cpp`.\n\nNavigate to the `whisper.cpp` directory.\n\n```bash\ncd /path/to/whisper.cpp\n```\n\nPull the latest changes and recompile the project.\n\n```bash\ngit pull --rebase\nrm -rf build/\ncmake -B build -DGGML_CUDA=1\ncmake --build build -j --config Release\n```\n\nCopy the updated programs to your system path.\n\n```bash\nsudo cp build/bin/whisper-server /usr/local/bin/\nsudo cp build/bin/whisper-cli /usr/local/bin/\n```\n\n### faster-whisper\n\nSince `faster-whisper` was installed as a Python package, updating it is simple.\n\nFirst, activate the Python environment you used for the installation.\n\n```bash\n# For Conda\nconda activate faster_whisper\n\n# For venv\nsource .faster_whisper/bin/activate\n```\n\nThen, use `pip` to upgrade the package to the latest version.\n\n```bash\npip install --upgrade faster-whisper\n```\n\n## Conclusion\n\n_Photo by [Vlad Bagacian](https://unsplash.com/@vladbagacian?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash) on [Unsplash](https://unsplash.com/photos/woman-sitting-on-grey-cliff-d1eaoAabeXs?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)._\n\nThis article documented my complete journey of replacing ollama with a more powerful, flexible, and fully-controlled local AI stack. We started with the goal of running large models that ollama couldnâ€™t handle and ended up building a complete system centered around `llama.cpp` and `llama-swap`.\n\nWe covered every step in detail, from building `llama.cpp` from source with **CUDA** support to serving large language models, embedding models, and multimodal models that can understand both images and audio. We integrated everything with **LibreChat**, providing a flexible interface that makes it easy to interact with our models.\n\nAn essential component of this process was `llama-swap`, which acted as an intelligent manager for our models. It automatically handles loading and unloading them on demand, which frees up VRAM and makes switching between different models seamless. When we faced challenges with audio input in LibreChat, we built a custom `faster-whisper` server to solve the audio format incompatibility.\n\nBeyond just running models, we focused on optimization and automation. We used `llama-bench` to fine-tune performance for our specific hardware and created specialized configurations for different context window sizes. We automated the entire workflow by creating `systemd` services to run everything in the background, desktop shortcuts for easy access, and a file watcher that automatically restarts services whenever we change a configuration file.\n\nWhat I ended up with is a setup that gives me full control over the entire stack. Itâ€™s a powerful, efficient, and customized system that has transformed how I interact with local models. I am extremely satisfied with this new environment and hope this guide helps you to build your own.\n\n## Updates\n\n### Solving the whisper audio format issue\n\nAfter publishing this article, I received a comment from a reader named [Gtinjr](https://medium.com/u/66a72706c889) on Medium. They pointed out that I gave up on `whisper.cpp` too early!\n\nIn the **Speech-to-Text with Whisper** section, I mentioned that I couldnâ€™t get the native `whisper-server` to work with LibreChat because of audio format incompatibilities (WebM vs WAV). I resorted to building a custom Python server using `faster-whisper` to bridge that gap.\n\nIt turns out I was missing two things: enabling FFmpeg during the build process AND using the `--convert` flag when running the server.\n\n> [!NOTE]\n> This FFmpeg integration is currently supported on **Linux only**.\n\nIf you prefer to use the native `whisper.cpp` server instead of the Python workaround, here is how to do it.\n\nYou need these libraries so `whisper.cpp` can link against them during compilation.\n\n```bash\nsudo apt install libavcodec-dev libavformat-dev libavutil-dev\n```\n\nNavigate to your `whisper.cpp` directory. You need to add `-DWHISPER_FFMPEG=yes` to your cmake command.\n\n> [!IMPORTANT]\n> In the command below, I am using `-DCMAKE_CUDA_ARCHITECTURES=89` which targets my specific GPU (RTX 4070). You must change this number to match your GPU's [compute capability](https://developer.nvidia.com/cuda/gpus) (e.g., `86` for RTX 3000 series, `75` for RTX 2000 series), or use `native` to let CMake detect it automatically.\n\n```bash\ncd ~/Programs/whisper.cpp\nrm -rf build/\n\n# Build with CUDA and FFmpeg support\ncmake -B build -DGGML_CUDA=1 -DWHISPER_FFMPEG=yes -DCMAKE_CUDA_ARCHITECTURES=89\ncmake --build build -j --config Release\n```\n\nNow you donâ€™t need the Python script or the virtual environment anymore. You can run the native server directly.\n\nOpen your `llama-swap/config.yaml` and update the whisper entry. The key change here is adding the `--convert` flag, which tells the server to automatically convert incoming audio (like WebM from LibreChat) into the WAV format it needs.\n\n```yaml\n\"whisper-large-v3-turbo\":\n  cmd: |\n    whisper-server\n    --model /home/imad-saddik/.cache/llama.cpp/whisper-large-v3-turbo-q8_0.gguf\n    --port ${PORT}\n    --host 0.0.0.0\n    --convert # <--- The flag that fixes the issue!\n  checkEndpoint: /health\n  ttl: 300\n```\n\nRestart llama-swap.\n\n```bash\nsudo systemctl restart llama-swap\n```\n\nNow, when you speak into LibreChat, `whisper-server` will accept the audio, convert it internally using FFmpeg, and return the transcription.\n\nThis solution is much cleaner because it removes the need for the external Python environment and the `faster-whisper` dependency. A huge thank you to [Gtinjr](https://medium.com/u/66a72706c889) for sharing this solution!\n\n#### Optional: Cleanup old faster-whisper files\n\nIf you previously followed the `faster-whisper` instructions, you can now safely remove the old files to free up space.\n\nStart by removing the Python project.\n\n```bash\nrm -rf ~/Programs/faster-whisper\n```\n\nNext, remove the Python environment.\n\n```bash\n# If you used Anaconda\nconda env remove -n faster_whisper\n\n# If you used venv\nrm -rf ~/.faster_whisper\n```\n\nMost importantly, donâ€™t forget to delete the cached model weights, as `faster-whisper` stores them separately from `llama.cpp` models.\n\n```bash\nrm -rf ~/.cache/huggingface/hub/models--Systran--faster-whisper-large-v3\n```\n\n### llama.cpp: New built-in WebUI\n\nRecently, `llama.cpp` [released a major update](https://github.com/ggml-org/llama.cpp/discussions/16938) that includes a new, modern WebUI built with [SvelteKit](https://svelte.dev/). It is a fully featured interface that runs directly from the `llama-server` binary without any extra installation.\n\nSome of the nice features include:\n\n- **Multimodal support:** You can upload images for vision models.\n- **Document processing:** You can upload text files and even PDFs directly into the chat context.\n- **Parallel conversations:** You can run multiple chat streams at the same time.\n- **Math rendering:** It supports LaTeX for rendering mathematical expressions.\n\nFor a full list of features and videos on how to use them, check out the [official discussion on GitHub](https://github.com/ggml-org/llama.cpp/discussions/16938).\n\nTo use it, you need to run `llama-server` directly. I recommend adding the `--jinja` flag, which enables the Jinja2 template engine to handle chat templates more accurately.\n\n```bash\nllama-server \\\n  --model ~/.cache/llama.cpp/Qwen3-30B-A3B-Thinking-2507-IQ4_XS.gguf \\\n  --port 8033 \\\n  --n-gpu-layers 19 \\\n  --ctx-size 4096 \\\n  --host 0.0.0.0 \\\n  --jinja\n```\n\nNow, navigate to [http://localhost:8033](https://www.google.com/search?q=http://localhost:8033) in your browser.\n\n_The new llama.cpp WebUI._\n",
    "view_count": 29,
    "read_count": 3,
    "claps_count": 6
  }
]

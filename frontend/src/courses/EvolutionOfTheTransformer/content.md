# Evolution of the Transformer architecture from 2017 to 2025

Discover how the Transformer architecture has evolved over the years. Implement the different ideas that researchers proposed to improve the original Transformer architecture.

**Date:** September 26, 2025
**Tags:** LLM, Transformer, Attention, Positional encoding, Normalization, PyTorch, Python, AI, NLP, Machine learning

---

## Course Overview

This course explores how the [Transformer architecture](<https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)>) has changed from its introduction in the famous paper [Attention is all you need](https://arxiv.org/pdf/1706.03762) in 2017 to the latest updates in 2025. We will look at the main ideas that researchers have used to improve the original Transformer, and you will learn how to build these improvements step by step.

You will learn both the theory and the practical side. We will cover different ways to encode the position of tokens, new [attention mechanisms](<https://en.wikipedia.org/wiki/Attention_(machine_learning)>), [activation functions](https://en.wikipedia.org/wiki/Activation_function), [normalization methods](<https://en.wikipedia.org/wiki/Normalization_(machine_learning)>), and other ideas that have helped make Transformers better.

You will use [Python](https://www.python.org/downloads/) and [PyTorch](https://pytorch.org/) to implement everything. I explain each idea in simple terms so anyone can follow along. All course materials, slides, and Jupyter notebooks are available on my [GitHub repository](https://github.com/ImadSaddik/Train_Your_Language_Model_Course).

By the end of the course, you will build several models using the ideas we discuss. We will compare their performance to see which ones work best. This will help us understand which improvements are most effective.

---

## Watch the Course

I believe knowledge should be free, so I am sharing this course for free on the freeCodeCamp YouTube channel. You can watch the full course below.

Creating courses like this takes months of research, experiments, designing illustrations, recording, and more.
If you want to support my mission to share free knowledge, you can [sponsor me on GitHub](https://github.com/sponsors/ImadSaddik) or [support me on Patreon](https://www.patreon.com/3CodeCamp).
